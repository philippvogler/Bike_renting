{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capstone Project\n",
    "Machine Learning Engineer Nanodegree\n",
    "\n",
    "# Demand prediction for a bike sharing systems\n",
    "\n",
    "July 2016  \n",
    "Philipp Vogler  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview  \n",
    "\n",
    "- A Problem in the area of transport and logistics that is solvable with machine learning.\n",
    "- Utilizing machine learning to forecast the demand for the Washington DC bike sharing system 'capital bike share'. \n",
    "- Using different types of regression to find an algorithm to predict the demand for bikes based on calendric and weather information.  \n",
    "- Weather, calendar and demand information is provided in a dataset by the University of Porto at UCI ML Repository.  \n",
    "- This project tries to create a forecasting function based on two years of historical data by utilizing the machine learning libraries scikit-learn and tensor-flow.  \n",
    "\n",
    "> http://www.capitalbikeshare.com   \n",
    "> http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset  \n",
    "> http://freemeteo.de/wetter/  \n",
    "> http://dchr.dc.gov/page/holiday-schedules  \n",
    "> http://scikit-learn.org/stable/  \n",
    "> https://www.tensorflow.org  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement  \n",
    "\n",
    "The goal is to forecast the demand for bikes in dependency of weather conditions like outside temperature and calendric informations e.g. holidays. These information and the demand structure is provided in a set with two years of daily historic data.  \n",
    "The demand is given as the total daily demand and as a split for registered users and casual users. To increase the quality of the prediction registered user demand and casual user demand will be predicted separately in step two.  \n",
    "To make predictions machine learning is used to train regressors. Scikit-Learn recommends a support vector regressor (SVR) for this kind of problem and data amount. In addition a deep neuronal network (DNN) regressor is trained for comparison. To find the hyper-parameters for these regressors grid search and randomized search are utilized. Due to the small dataset cross validation is applied.    \n",
    "\n",
    "> http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR  \n",
    "> https://github.com/tensorflow/skflow/blob/master/g3doc/api_docs/python/estimators.md  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "To measure the performance of the regressions two standard regression metrics are used: Mean squared eror (MSE) and the coefficient of determination (R^2). Both metrics are calculated for both regressor types. For comparison and parameter tuning only R^2 is used due to the better readability.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from tensorflow.contrib import skflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fetching Dataset\n",
    "\n",
    "bike_data = pd.read_csv(\"day.csv\")\n",
    "\n",
    "print \"Data read successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):\n",
      "Index([u'instant', u'dteday', u'season', u'yr', u'mnth', u'holiday',\n",
      "       u'weekday', u'workingday', u'weathersit', u'temp', u'atemp', u'hum',\n",
      "       u'windspeed'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column:\n",
      "cnt\n"
     ]
    }
   ],
   "source": [
    "# Extracting\n",
    "\n",
    "feature_cols = bike_data.columns[:-3]  # all columns but last are features\n",
    "target_col = bike_data.columns[-1]  # last column is the target\n",
    "\n",
    "print (\"Feature column(s):\\n{}\\n\".format(feature_cols))\n",
    "print (\"Target column:\\n{}\".format(target_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Data values:\n",
      "   instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01       1   0     1        0        6           0   \n",
      "1        2  2011-01-02       1   0     1        0        0           0   \n",
      "2        3  2011-01-03       1   0     1        0        1           1   \n",
      "3        4  2011-01-04       1   0     1        0        2           1   \n",
      "4        5  2011-01-05       1   0     1        0        3           1   \n",
      "\n",
      "   weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
      "0           2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
      "1           2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
      "2           1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
      "3           1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
      "4           1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
      "\n",
      "    cnt  \n",
      "0   985  \n",
      "1   801  \n",
      "2  1349  \n",
      "3  1562  \n",
      "4  1600  \n",
      "\n",
      " Data stats:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>366.000000</td>\n",
       "      <td>2.496580</td>\n",
       "      <td>0.500684</td>\n",
       "      <td>6.519836</td>\n",
       "      <td>0.028728</td>\n",
       "      <td>2.997264</td>\n",
       "      <td>0.683995</td>\n",
       "      <td>1.395349</td>\n",
       "      <td>0.495385</td>\n",
       "      <td>0.474354</td>\n",
       "      <td>0.627894</td>\n",
       "      <td>0.190486</td>\n",
       "      <td>848.176471</td>\n",
       "      <td>3656.172367</td>\n",
       "      <td>4504.348837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>211.165812</td>\n",
       "      <td>1.110807</td>\n",
       "      <td>0.500342</td>\n",
       "      <td>3.451913</td>\n",
       "      <td>0.167155</td>\n",
       "      <td>2.004787</td>\n",
       "      <td>0.465233</td>\n",
       "      <td>0.544894</td>\n",
       "      <td>0.183051</td>\n",
       "      <td>0.162961</td>\n",
       "      <td>0.142429</td>\n",
       "      <td>0.077498</td>\n",
       "      <td>686.622488</td>\n",
       "      <td>1560.256377</td>\n",
       "      <td>1937.211452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059130</td>\n",
       "      <td>0.079070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022392</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>183.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337083</td>\n",
       "      <td>0.337842</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.134950</td>\n",
       "      <td>315.500000</td>\n",
       "      <td>2497.000000</td>\n",
       "      <td>3152.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>366.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498333</td>\n",
       "      <td>0.486733</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.180975</td>\n",
       "      <td>713.000000</td>\n",
       "      <td>3662.000000</td>\n",
       "      <td>4548.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>548.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.655417</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>0.730209</td>\n",
       "      <td>0.233214</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>4776.500000</td>\n",
       "      <td>5956.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.861667</td>\n",
       "      <td>0.840896</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>3410.000000</td>\n",
       "      <td>6946.000000</td>\n",
       "      <td>8714.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          instant      season          yr        mnth     holiday     weekday  \\\n",
       "count  731.000000  731.000000  731.000000  731.000000  731.000000  731.000000   \n",
       "mean   366.000000    2.496580    0.500684    6.519836    0.028728    2.997264   \n",
       "std    211.165812    1.110807    0.500342    3.451913    0.167155    2.004787   \n",
       "min      1.000000    1.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "25%    183.500000    2.000000    0.000000    4.000000    0.000000    1.000000   \n",
       "50%    366.000000    3.000000    1.000000    7.000000    0.000000    3.000000   \n",
       "75%    548.500000    3.000000    1.000000   10.000000    0.000000    5.000000   \n",
       "max    731.000000    4.000000    1.000000   12.000000    1.000000    6.000000   \n",
       "\n",
       "       workingday  weathersit        temp       atemp         hum   windspeed  \\\n",
       "count  731.000000  731.000000  731.000000  731.000000  731.000000  731.000000   \n",
       "mean     0.683995    1.395349    0.495385    0.474354    0.627894    0.190486   \n",
       "std      0.465233    0.544894    0.183051    0.162961    0.142429    0.077498   \n",
       "min      0.000000    1.000000    0.059130    0.079070    0.000000    0.022392   \n",
       "25%      0.000000    1.000000    0.337083    0.337842    0.520000    0.134950   \n",
       "50%      1.000000    1.000000    0.498333    0.486733    0.626667    0.180975   \n",
       "75%      1.000000    2.000000    0.655417    0.608602    0.730209    0.233214   \n",
       "max      1.000000    3.000000    0.861667    0.840896    0.972500    0.507463   \n",
       "\n",
       "            casual   registered          cnt  \n",
       "count   731.000000   731.000000   731.000000  \n",
       "mean    848.176471  3656.172367  4504.348837  \n",
       "std     686.622488  1560.256377  1937.211452  \n",
       "min       2.000000    20.000000    22.000000  \n",
       "25%     315.500000  2497.000000  3152.000000  \n",
       "50%     713.000000  3662.000000  4548.000000  \n",
       "75%    1096.000000  4776.500000  5956.000000  \n",
       "max    3410.000000  6946.000000  8714.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploration\n",
    "\n",
    "print \"\\n Data values:\"\n",
    "print bike_data.head()  # print the first 5 rows\n",
    "\n",
    "print \"\\n Data stats:\"\n",
    "bike_data.describe() # shows stats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics\n",
    "\n",
    "- The dataset is very concise and missing values are not a problem.   \n",
    "- Most of the data is already normalized or binary.  \n",
    "- Categorical data like 'weekday' or 'working day' are already processed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "\n",
    "The visualization shows a classic seasonal pattern with an up trend year over year. There are some outliers. These are left in the dataset because they are not due to measurement errors, but to extreme weather conditions. Because extreme weather conditions are part of the problem, so the data is not excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visulazation\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(1)\n",
    "      \n",
    "plt.plot(bike_data.instant[:len(bike_data)/2], bike_data.cnt[:len(bike_data)/2] ,'go', label='2011')\n",
    "plt.plot(bike_data.instant[:(len(bike_data)/2)+1], bike_data.cnt[len(bike_data)/2:], 'ro', label='2012')\n",
    "\n",
    "plt.title('Number of bikes rented per day')\n",
    "\n",
    "plt.xlabel('Days')\n",
    "plt.xticks((np.arange(0,len(bike_data)/2,len(bike_data)/24)), calendar.month_name[1:13], rotation=45)\n",
    "\n",
    "plt.ylabel('Number of bikes')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# source: http://matplotlib.org/examples/showcase/bachelors_degrees_by_gender.html\n",
    "# source: http://matplotlib.org/api/pyplot_api.html?highlight=xticks#matplotlib.pyplot.xticks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing (Methodology)\n",
    "\n",
    "Dates get dropped because the regressor can not read this datatype and the information is already stored in the 'mnth' and 'yr' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "\n",
    "X = bike_data[feature_cols.drop(['dteday'],['instant'])] # feature values \n",
    "y = bike_data[target_col]  # corresponding targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)# test size is set to 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of regressors are trained. An SVR and a DNN-Regressor. Both are first used off-the-shelf with default parameters to create a benchmark.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR  \n",
    "> https://github.com/tensorflow/skflow/blob/master/g3doc/api_docs/python/estimators.md  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both \"benchmarks\" for the coefficient of determination are very low. Parameter tuning is mandatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training SVR\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score SVR: 0.001379\n"
     ]
    }
   ],
   "source": [
    "# Validation SVR\n",
    "\n",
    "svr_pred = svr.predict(X_test)\n",
    "\n",
    "# score_svr = mean_squared_error(y_test, svr_pred)\n",
    "score_svr = r2_score(y_test, svr_pred)\n",
    "\n",
    "print(\"Score SVR: %f\" % score_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #99, avg. train loss: 2888112.25000\n",
      "Step #199, avg. train loss: 2483481.00000\n",
      "Step #299, avg. train loss: 1777717.25000\n",
      "Step #399, avg. train loss: 2226688.75000\n",
      "Step #499, avg. train loss: 2371832.25000\n",
      "Step #600, epoch #1, avg. train loss: 2154336.50000\n",
      "Step #700, epoch #1, avg. train loss: 2409113.00000\n",
      "Step #800, epoch #1, avg. train loss: 2185903.50000\n",
      "Step #900, epoch #1, avg. train loss: 2421249.50000\n",
      "Step #1000, epoch #1, avg. train loss: 1998601.62500\n",
      "Step #1100, epoch #2, avg. train loss: 1684017.50000\n",
      "Step #1200, epoch #2, avg. train loss: 2781210.50000\n",
      "Step #1300, epoch #2, avg. train loss: 2356811.75000\n",
      "Step #1400, epoch #2, avg. train loss: 1902917.12500\n",
      "Step #1500, epoch #2, avg. train loss: 2152630.75000\n",
      "Step #1600, epoch #2, avg. train loss: 2017118.87500\n",
      "Step #1700, epoch #3, avg. train loss: 1538977.00000\n",
      "Step #1800, epoch #3, avg. train loss: 2071140.62500\n",
      "Step #1900, epoch #3, avg. train loss: 2359790.50000\n",
      "Step #2000, epoch #3, avg. train loss: 2115904.00000\n",
      "Step #2100, epoch #3, avg. train loss: 1597071.62500\n",
      "Step #2200, epoch #4, avg. train loss: 1825081.75000\n",
      "Step #2300, epoch #4, avg. train loss: 1874912.37500\n",
      "Step #2400, epoch #4, avg. train loss: 2290931.00000\n",
      "Step #2500, epoch #4, avg. train loss: 2212923.75000\n",
      "Step #2600, epoch #4, avg. train loss: 1747889.50000\n",
      "Step #2700, epoch #4, avg. train loss: 2441345.50000\n",
      "Step #2800, epoch #5, avg. train loss: 2116986.75000\n",
      "Step #2900, epoch #5, avg. train loss: 2039761.50000\n",
      "Step #3000, epoch #5, avg. train loss: 1978833.87500\n",
      "Step #3100, epoch #5, avg. train loss: 2036103.87500\n",
      "Step #3200, epoch #5, avg. train loss: 1580222.37500\n",
      "Step #3300, epoch #6, avg. train loss: 1746733.87500\n",
      "Step #3400, epoch #6, avg. train loss: 2472411.00000\n",
      "Step #3500, epoch #6, avg. train loss: 1916774.25000\n",
      "Step #3600, epoch #6, avg. train loss: 1640949.25000\n",
      "Step #3700, epoch #6, avg. train loss: 1588597.50000\n",
      "Step #3800, epoch #6, avg. train loss: 2061549.12500\n",
      "Step #3900, epoch #7, avg. train loss: 2205547.00000\n",
      "Step #4000, epoch #7, avg. train loss: 2169178.25000\n",
      "Step #4100, epoch #7, avg. train loss: 2155737.75000\n",
      "Step #4200, epoch #7, avg. train loss: 1526676.37500\n",
      "Step #4300, epoch #7, avg. train loss: 1756006.50000\n",
      "Step #4400, epoch #8, avg. train loss: 2181028.75000\n",
      "Step #4500, epoch #8, avg. train loss: 1625820.50000\n",
      "Step #4600, epoch #8, avg. train loss: 2241140.25000\n",
      "Step #4700, epoch #8, avg. train loss: 2084310.25000\n",
      "Step #4800, epoch #8, avg. train loss: 1997853.62500\n",
      "Step #4900, epoch #8, avg. train loss: 1961454.25000\n",
      "Step #5000, epoch #9, avg. train loss: 1852030.50000\n",
      "\n",
      " Score: -0.488562\n"
     ]
    }
   ],
   "source": [
    "# DNN-Regressor\n",
    "\n",
    "# Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
    "regressor = skflow.TensorFlowDNNRegressor(hidden_units=[10,10], steps=5000, learning_rate=0.1, batch_size=1)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and validate\n",
    "#score_regressor = metrics.mean_squared_error( y_test, regressor.predict(X_test))\n",
    "score_regressor = r2_score(y_test, regressor.predict(X_test))\n",
    "\n",
    "print('\\n Score: {0:f}'.format(score_regressor))\n",
    "\n",
    "# Copyright 2015-present The Scikit Flow Authors. All Rights Reserved.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# source https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/boston.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The regressors are trained using randomized search and cross-validation to identify the area of the best parameters. Then a grid search is used to tune parameter values of the regressor functions.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid=[{'kernel': ['linear', 'rbf'], 'C': [1000, 3000, 10000]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)\n",
      "\n",
      "Best parameter from grid search: {'kernel': 'linear', 'C': 1000}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tuning SVR with GridSearch\n",
    "\n",
    "tuned_parameters = [{'C': [1000, 3000, 10000], \n",
    "                     'kernel': ['linear', 'rbf']}\n",
    "                   ]\n",
    "\n",
    "#svr_tuned = GridSearchCV(SVR (C=1), param_grid = tuned_parameters, scoring = 'mean_squared_error') #default 3-fold cross-validation, score method of the estimator\n",
    "svr_tuned_GS = GridSearchCV(SVR (C=1), param_grid = tuned_parameters, scoring = 'r2', n_jobs=-1) #default 3-fold cross-validation, score method of the estimator\n",
    "\n",
    "svr_tuned_GS.fit(X_train, y_train)\n",
    "\n",
    "print (svr_tuned_GS)\n",
    "print ('\\n' \"Best parameter from grid search: \" + str(svr_tuned_GS.best_params_) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Results\n",
      "\n",
      "Score SVR: 0.001379\n",
      "Score SVR tuned GS: 0.778184\n"
     ]
    }
   ],
   "source": [
    "# Validation - SVR tuned \n",
    "\n",
    "svr_tuned_pred_GS = svr_tuned_GS.predict(X_test)\n",
    "\n",
    "#score_svr_tuned = mean_squared_error(y_test, svr_tuned_pred)\n",
    "score_svr_tuned_GS = r2_score(y_test, svr_tuned_pred_GS)\n",
    "\n",
    "print('SVR Results\\n')\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"Score SVR tuned GS: %f\" % score_svr_tuned_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best CV score from grid search: 0.786727\n",
      "corresponding parameters: {'kernel': 'linear', 'C': 4959.560207749184}\n"
     ]
    }
   ],
   "source": [
    "# SVR tuned with RandomizesSearch\n",
    "# may take a while!\n",
    "\n",
    "# Parameters\n",
    "param_dist = {  'C': sp_uniform (1000, 10000), \n",
    "                'kernel': ['linear']\n",
    "             }\n",
    "\n",
    "n_iter_search = 1\n",
    "\n",
    "# MSE optimized\n",
    "#SVR_tuned_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'mean_squared_error', n_iter=n_iter_search)\n",
    "\n",
    "# R^2 optimized\n",
    "SVR_tuned_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'r2', n_iter=n_iter_search)\n",
    "\n",
    "# Fit\n",
    "SVR_tuned_RS.fit(X_train, y_train)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(SVR_tuned_RS.best_score_))\n",
    "print('corresponding parameters: {}'.format(SVR_tuned_RS.best_params_))\n",
    "\n",
    "# Predict and score\n",
    "predict = SVR_tuned_RS.predict(X_test)\n",
    "\n",
    "#score_regressor_tuned_RS = mean_squared_error(y_test, predict)\n",
    "score_svr_tuned_RS = r2_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Results\n",
      "\n",
      "Score SVR: 0.001379\n",
      "Score SVR tuned GS: 0.778184\n",
      "Score SVR tuned RS: 0.775381\n"
     ]
    }
   ],
   "source": [
    "print('SVR Results\\n')\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"Score SVR tuned GS: %f\" % score_svr_tuned_GS)\n",
    "print(\"Score SVR tuned RS: %f\" % score_svr_tuned_RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning works for the SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100, epoch #50, avg. train loss: 2575044.50000\n",
      "Step #100, epoch #50, avg. train loss: 2335793.50000\n",
      "Step #100, epoch #50, avg. train loss: 2475758.75000\n",
      "Step #100, epoch #50, avg. train loss: 2199084.25000\n",
      "Step #100, epoch #50, avg. train loss: 1939909.75000\n",
      "Step #100, epoch #50, avg. train loss: 2062815.87500\n",
      "Step #100, epoch #50, avg. train loss: 2748201.25000\n",
      "Step #100, epoch #50, avg. train loss: 2468335.00000\n",
      "Step #100, epoch #50, avg. train loss: 2603180.50000\n",
      "Step #100, epoch #50, avg. train loss: 2880148.75000\n",
      "Step #100, epoch #50, avg. train loss: 2436922.25000\n",
      "Step #100, epoch #50, avg. train loss: 2745935.75000\n",
      "Step #100, epoch #50, avg. train loss: 2612682.00000\n",
      "Step #100, epoch #50, avg. train loss: 2380895.25000\n",
      "Step #100, epoch #50, avg. train loss: 2521312.25000\n",
      "Step #100, epoch #50, avg. train loss: 2193559.25000\n",
      "Step #100, epoch #50, avg. train loss: 1937217.62500\n",
      "Step #100, epoch #50, avg. train loss: 2067234.50000\n",
      "Step #100, epoch #50, avg. train loss: 2699485.75000\n",
      "Step #100, epoch #50, avg. train loss: 2368371.75000\n",
      "Step #100, epoch #50, avg. train loss: 2550067.25000\n",
      "Step #100, epoch #50, avg. train loss: 3002579.50000\n",
      "Step #100, epoch #50, avg. train loss: 2764092.25000\n",
      "Step #100, epoch #50, avg. train loss: 2876087.75000\n",
      "Step #100, epoch #50, avg. train loss: 2733023.00000\n",
      "Step #100, epoch #50, avg. train loss: 2477811.50000\n",
      "Step #100, epoch #50, avg. train loss: 2627444.00000\n",
      "Step #100, epoch #50, avg. train loss: 2245354.75000\n",
      "Step #100, epoch #50, avg. train loss: 2000829.87500\n",
      "Step #100, epoch #50, avg. train loss: 2087905.12500\n",
      "Step #100, epoch #50, avg. train loss: 2320290.75000\n",
      "Step #100, epoch #50, avg. train loss: 2069001.00000\n",
      "Step #100, epoch #50, avg. train loss: 2231643.00000\n",
      "Step #100, epoch #50, avg. train loss: 3922894.00000\n",
      "Step #100, epoch #50, avg. train loss: 3628896.75000\n",
      "Step #100, epoch #50, avg. train loss: 3745654.00000\n",
      "Step #100, epoch #50, avg. train loss: 2405958.25000\n",
      "Step #100, epoch #50, avg. train loss: 2152465.00000\n",
      "Step #100, epoch #50, avg. train loss: 2286351.25000\n",
      "Step #100, epoch #50, avg. train loss: 2205911.00000\n",
      "Step #100, epoch #50, avg. train loss: 1945645.75000\n",
      "Step #100, epoch #50, avg. train loss: 2013723.62500\n",
      "Step #100, epoch #50, avg. train loss: 3045420.25000\n",
      "Step #100, epoch #50, avg. train loss: 2739833.50000\n",
      "Step #100, epoch #50, avg. train loss: 2901580.50000\n",
      "Step #100, epoch #50, avg. train loss: 3130541.75000\n",
      "Step #100, epoch #50, avg. train loss: 2902418.50000\n",
      "Step #100, epoch #50, avg. train loss: 2849260.25000\n",
      "Step #100, epoch #50, avg. train loss: 2485231.75000\n",
      "Step #100, epoch #50, avg. train loss: 2244500.00000\n",
      "Step #100, epoch #50, avg. train loss: 2374365.25000\n",
      "Step #100, epoch #50, avg. train loss: 2200309.00000\n",
      "Step #100, epoch #50, avg. train loss: 1956846.87500\n",
      "Step #100, epoch #50, avg. train loss: 2044337.62500\n",
      "Step #100, epoch #50, avg. train loss: 3049907.50000\n",
      "Step #100, epoch #50, avg. train loss: 2800106.00000\n",
      "Step #100, epoch #50, avg. train loss: 3000652.50000\n",
      "Step #100, epoch #50, avg. train loss: 2737227.75000\n",
      "Step #100, epoch #50, avg. train loss: 3006990.50000\n",
      "Step #100, epoch #50, avg. train loss: 2672715.25000\n",
      "Step #100, epoch #50, avg. train loss: 2568484.25000\n",
      "Step #100, epoch #50, avg. train loss: 2320617.00000\n",
      "Step #100, epoch #50, avg. train loss: 2473323.50000\n",
      "Step #100, epoch #50, avg. train loss: 2184789.00000\n",
      "Step #100, epoch #50, avg. train loss: 1935490.12500\n",
      "Step #100, epoch #50, avg. train loss: 2056939.25000\n",
      "Step #100, epoch #50, avg. train loss: 2726156.75000\n",
      "Step #100, epoch #50, avg. train loss: 2384026.50000\n",
      "Step #100, epoch #50, avg. train loss: 2676381.50000\n",
      "Step #100, epoch #50, avg. train loss: 2869358.50000\n",
      "Step #100, epoch #50, avg. train loss: 2577792.50000\n",
      "Step #100, epoch #50, avg. train loss: 2622977.25000\n",
      "Step #100, epoch #50, avg. train loss: 2619073.25000\n",
      "Step #100, epoch #50, avg. train loss: 2374478.50000\n",
      "Step #100, epoch #50, avg. train loss: 2526221.75000\n",
      "Step #100, epoch #50, avg. train loss: 2178799.00000\n",
      "Step #100, epoch #50, avg. train loss: 1920685.87500\n",
      "Step #100, epoch #50, avg. train loss: 2060258.50000\n",
      "Step #100, epoch #50, avg. train loss: 2628412.75000\n",
      "Step #100, epoch #50, avg. train loss: 2283640.75000\n",
      "Step #100, epoch #50, avg. train loss: 2516092.75000\n",
      "Step #100, epoch #50, avg. train loss: 2994075.75000\n",
      "Step #100, epoch #50, avg. train loss: 2825467.00000\n",
      "Step #100, epoch #50, avg. train loss: 2863025.50000\n",
      "Step #100, epoch #50, avg. train loss: 2714115.25000\n",
      "Step #100, epoch #50, avg. train loss: 2464133.00000\n",
      "Step #100, epoch #50, avg. train loss: 2627884.00000\n",
      "Step #100, epoch #50, avg. train loss: 2220339.25000\n",
      "Step #100, epoch #50, avg. train loss: 1971540.62500\n",
      "Step #100, epoch #50, avg. train loss: 2076396.12500\n",
      "Step #100, epoch #50, avg. train loss: 2352466.00000\n",
      "Step #100, epoch #50, avg. train loss: 2073554.87500\n",
      "Step #100, epoch #50, avg. train loss: 2186012.25000\n",
      "Step #100, epoch #50, avg. train loss: 4041613.50000\n",
      "Step #100, epoch #50, avg. train loss: 3672321.00000\n",
      "Step #100, epoch #50, avg. train loss: 3766251.50000\n",
      "Step #100, epoch #50, avg. train loss: 2387373.25000\n",
      "Step #100, epoch #50, avg. train loss: 2140857.50000\n",
      "Step #100, epoch #50, avg. train loss: 2282629.50000\n",
      "Step #100, epoch #50, avg. train loss: 2184562.25000\n",
      "Step #100, epoch #50, avg. train loss: 1924185.50000\n",
      "Step #100, epoch #50, avg. train loss: 2075768.75000\n",
      "Step #100, epoch #50, avg. train loss: 3041334.75000\n",
      "Step #100, epoch #50, avg. train loss: 2681793.75000\n",
      "Step #100, epoch #50, avg. train loss: 2930782.50000\n",
      "Step #100, epoch #50, avg. train loss: 3025116.75000\n",
      "Step #100, epoch #50, avg. train loss: 2750484.75000\n",
      "Step #100, epoch #50, avg. train loss: 2972791.00000\n",
      "Step #100, epoch #50, avg. train loss: 2479327.25000\n",
      "Step #100, epoch #50, avg. train loss: 2227906.50000\n",
      "Step #100, epoch #50, avg. train loss: 2373571.75000\n",
      "Step #100, epoch #50, avg. train loss: 2210319.25000\n",
      "Step #100, epoch #50, avg. train loss: 1941118.12500\n",
      "Step #100, epoch #50, avg. train loss: 2066710.37500\n",
      "Step #100, epoch #50, avg. train loss: 3175702.00000\n",
      "Step #100, epoch #50, avg. train loss: 2738160.75000\n",
      "Step #100, epoch #50, avg. train loss: 2964533.75000\n",
      "Step #100, epoch #50, avg. train loss: 2528948.25000\n",
      "Step #100, epoch #50, avg. train loss: 2598057.50000\n",
      "Step #100, epoch #50, avg. train loss: 2714357.75000\n",
      "Step #100, epoch #50, avg. train loss: 2617112.75000\n",
      "Step #100, epoch #50, avg. train loss: 2326508.75000\n",
      "Step #100, epoch #50, avg. train loss: 2474392.75000\n",
      "Step #100, epoch #50, avg. train loss: 2294982.25000\n",
      "Step #100, epoch #50, avg. train loss: 1964953.00000\n",
      "Step #100, epoch #50, avg. train loss: 2065625.50000\n",
      "Step #100, epoch #50, avg. train loss: 2928438.00000\n",
      "Step #100, epoch #50, avg. train loss: 2630249.25000\n",
      "Step #100, epoch #50, avg. train loss: 2508784.75000\n",
      "Step #100, epoch #50, avg. train loss: 2977235.75000\n",
      "Step #100, epoch #50, avg. train loss: 2729103.75000\n",
      "Step #100, epoch #50, avg. train loss: 2862695.75000\n",
      "Step #100, epoch #50, avg. train loss: 2656260.75000\n",
      "Step #100, epoch #50, avg. train loss: 2373135.25000\n",
      "Step #100, epoch #50, avg. train loss: 2534861.00000\n",
      "Step #100, epoch #50, avg. train loss: 2272335.00000\n",
      "Step #100, epoch #50, avg. train loss: 1973088.50000\n",
      "Step #100, epoch #50, avg. train loss: 2081810.75000\n",
      "Step #100, epoch #50, avg. train loss: 2791359.00000\n",
      "Step #100, epoch #50, avg. train loss: 2422698.25000\n",
      "Step #100, epoch #50, avg. train loss: 2633941.00000\n",
      "Step #100, epoch #50, avg. train loss: 3269884.25000\n",
      "Step #100, epoch #50, avg. train loss: 3031749.75000\n",
      "Step #100, epoch #50, avg. train loss: 2986112.00000\n",
      "Step #100, epoch #50, avg. train loss: 2761105.00000\n",
      "Step #100, epoch #50, avg. train loss: 2471928.25000\n",
      "Step #100, epoch #50, avg. train loss: 2633325.50000\n",
      "Step #100, epoch #50, avg. train loss: 2253429.50000\n",
      "Step #100, epoch #50, avg. train loss: 1955966.75000\n",
      "Step #100, epoch #50, avg. train loss: 2095589.87500\n",
      "Step #100, epoch #50, avg. train loss: 2339534.50000\n",
      "Step #100, epoch #50, avg. train loss: 2102055.75000\n",
      "Step #100, epoch #50, avg. train loss: 2188455.25000\n",
      "Step #100, epoch #50, avg. train loss: 4164295.75000\n",
      "Step #100, epoch #50, avg. train loss: 3582423.75000\n",
      "Step #100, epoch #50, avg. train loss: 3954555.50000\n",
      "Step #100, epoch #50, avg. train loss: 2465623.25000\n",
      "Step #100, epoch #50, avg. train loss: 2144875.00000\n",
      "Step #100, epoch #50, avg. train loss: 2281691.75000\n",
      "Step #100, epoch #50, avg. train loss: 2262199.50000\n",
      "Step #100, epoch #50, avg. train loss: 2023309.12500\n",
      "Step #100, epoch #50, avg. train loss: 2080264.50000\n",
      "Step #100, epoch #50, avg. train loss: 3160121.00000\n",
      "Step #100, epoch #50, avg. train loss: 2692563.50000\n",
      "Step #100, epoch #50, avg. train loss: 2880615.25000\n",
      "Step #100, epoch #50, avg. train loss: 3003653.75000\n",
      "Step #100, epoch #50, avg. train loss: 2779236.75000\n",
      "Step #100, epoch #50, avg. train loss: 3258361.25000\n",
      "Step #100, epoch #50, avg. train loss: 2549605.75000\n",
      "Step #100, epoch #50, avg. train loss: 2239579.25000\n",
      "Step #100, epoch #50, avg. train loss: 2367200.75000\n",
      "Step #100, epoch #50, avg. train loss: 2262063.75000\n",
      "Step #100, epoch #50, avg. train loss: 2007061.50000\n",
      "Step #100, epoch #50, avg. train loss: 2112646.00000\n",
      "Step #100, epoch #50, avg. train loss: 3313246.00000\n",
      "Step #100, epoch #50, avg. train loss: 2875789.50000\n",
      "Step #100, epoch #50, avg. train loss: 3118054.50000\n",
      "Step #100, epoch #50, avg. train loss: 2712651.75000\n",
      "Step #100, epoch #50, avg. train loss: 3082974.50000\n",
      "Step #100, epoch #50, avg. train loss: 2608321.75000\n",
      "Step #100, epoch #100, avg. train loss: 2535029.00000\n",
      "Step #100, epoch #100, avg. train loss: 2310741.00000\n",
      "Step #100, epoch #100, avg. train loss: 2452110.75000\n",
      "Step #100, epoch #100, avg. train loss: 2153114.25000\n",
      "Step #100, epoch #100, avg. train loss: 1926250.25000\n",
      "Step #100, epoch #100, avg. train loss: 2057720.75000\n",
      "Step #100, epoch #100, avg. train loss: 2677576.50000\n",
      "Step #100, epoch #100, avg. train loss: 2417764.75000\n",
      "Step #100, epoch #100, avg. train loss: 2560481.00000\n",
      "Step #100, epoch #100, avg. train loss: 2752296.00000\n",
      "Step #100, epoch #100, avg. train loss: 2482171.75000\n",
      "Step #100, epoch #100, avg. train loss: 2757356.50000\n",
      "Step #100, epoch #100, avg. train loss: 2584193.50000\n",
      "Step #100, epoch #100, avg. train loss: 2361046.00000\n",
      "Step #100, epoch #100, avg. train loss: 2504155.50000\n",
      "Step #100, epoch #100, avg. train loss: 2160097.50000\n",
      "Step #100, epoch #100, avg. train loss: 1912975.87500\n",
      "Step #100, epoch #100, avg. train loss: 2050770.75000\n",
      "Step #100, epoch #100, avg. train loss: 2615093.00000\n",
      "Step #100, epoch #100, avg. train loss: 2341596.00000\n",
      "Step #100, epoch #100, avg. train loss: 2484540.75000\n",
      "Step #100, epoch #100, avg. train loss: 3053512.00000\n",
      "Step #100, epoch #100, avg. train loss: 2716439.75000\n",
      "Step #100, epoch #100, avg. train loss: 2877277.00000\n",
      "Step #100, epoch #100, avg. train loss: 2669740.00000\n",
      "Step #100, epoch #100, avg. train loss: 2442155.75000\n",
      "Step #100, epoch #100, avg. train loss: 2591469.50000\n",
      "Step #100, epoch #100, avg. train loss: 2197206.50000\n",
      "Step #100, epoch #100, avg. train loss: 1964812.62500\n",
      "Step #100, epoch #100, avg. train loss: 2086939.37500\n",
      "Step #100, epoch #100, avg. train loss: 2327932.25000\n",
      "Step #100, epoch #100, avg. train loss: 2078456.12500\n",
      "Step #100, epoch #100, avg. train loss: 2209197.00000\n",
      "Step #100, epoch #100, avg. train loss: 3935593.25000\n",
      "Step #100, epoch #100, avg. train loss: 3600800.75000\n",
      "Step #100, epoch #100, avg. train loss: 3803316.25000\n",
      "Step #100, epoch #100, avg. train loss: 2367294.25000\n",
      "Step #100, epoch #100, avg. train loss: 2139623.75000\n",
      "Step #100, epoch #100, avg. train loss: 2267168.75000\n",
      "Step #100, epoch #100, avg. train loss: 2178367.00000\n",
      "Step #100, epoch #100, avg. train loss: 1952476.62500\n",
      "Step #100, epoch #100, avg. train loss: 2053988.00000\n",
      "Step #100, epoch #100, avg. train loss: 2925592.25000\n",
      "Step #100, epoch #100, avg. train loss: 2669351.25000\n",
      "Step #100, epoch #100, avg. train loss: 2820779.75000\n",
      "Step #100, epoch #100, avg. train loss: 2950780.50000\n",
      "Step #100, epoch #100, avg. train loss: 2775877.75000\n",
      "Step #100, epoch #100, avg. train loss: 3034507.25000\n",
      "Step #100, epoch #100, avg. train loss: 2444796.25000\n",
      "Step #100, epoch #100, avg. train loss: 2221740.25000\n",
      "Step #100, epoch #100, avg. train loss: 2352211.50000\n",
      "Step #100, epoch #100, avg. train loss: 2174454.50000\n",
      "Step #100, epoch #100, avg. train loss: 1951294.12500\n",
      "Step #100, epoch #100, avg. train loss: 2076313.00000\n",
      "Step #100, epoch #100, avg. train loss: 3109791.00000\n",
      "Step #100, epoch #100, avg. train loss: 2801481.00000\n",
      "Step #100, epoch #100, avg. train loss: 2876316.25000\n",
      "Step #100, epoch #100, avg. train loss: 2589545.75000\n",
      "Step #100, epoch #100, avg. train loss: 3138917.75000\n",
      "Step #100, epoch #100, avg. train loss: 2456370.50000\n",
      "Step #100, epoch #100, avg. train loss: 2535029.00000\n",
      "Step #100, epoch #100, avg. train loss: 2310741.00000\n",
      "Step #100, epoch #100, avg. train loss: 2452110.75000\n",
      "Step #100, epoch #100, avg. train loss: 2153114.25000\n",
      "Step #100, epoch #100, avg. train loss: 1926250.25000\n",
      "Step #100, epoch #100, avg. train loss: 2057720.75000\n",
      "Step #100, epoch #100, avg. train loss: 2677576.50000\n",
      "Step #100, epoch #100, avg. train loss: 2417764.75000\n",
      "Step #100, epoch #100, avg. train loss: 2560481.00000\n",
      "Step #100, epoch #100, avg. train loss: 2752296.00000\n",
      "Step #100, epoch #100, avg. train loss: 2482171.75000\n",
      "Step #100, epoch #100, avg. train loss: 2757356.50000\n",
      "Step #100, epoch #100, avg. train loss: 2584193.50000\n",
      "Step #100, epoch #100, avg. train loss: 2361046.00000\n",
      "Step #100, epoch #100, avg. train loss: 2504155.50000\n",
      "Step #100, epoch #100, avg. train loss: 2160097.50000\n",
      "Step #100, epoch #100, avg. train loss: 1912975.87500\n",
      "Step #100, epoch #100, avg. train loss: 2050770.75000\n",
      "Step #100, epoch #100, avg. train loss: 2615093.00000\n",
      "Step #100, epoch #100, avg. train loss: 2341596.00000\n",
      "Step #100, epoch #100, avg. train loss: 2484540.75000\n",
      "Step #100, epoch #100, avg. train loss: 3053512.00000\n",
      "Step #100, epoch #100, avg. train loss: 2716439.75000\n",
      "Step #100, epoch #100, avg. train loss: 2877277.00000\n",
      "Step #100, epoch #100, avg. train loss: 2669740.00000\n",
      "Step #100, epoch #100, avg. train loss: 2442155.75000\n",
      "Step #100, epoch #100, avg. train loss: 2591469.50000\n",
      "Step #100, epoch #100, avg. train loss: 2197206.50000\n",
      "Step #100, epoch #100, avg. train loss: 1964812.62500\n",
      "Step #100, epoch #100, avg. train loss: 2086939.37500\n",
      "Step #100, epoch #100, avg. train loss: 2327932.25000\n",
      "Step #100, epoch #100, avg. train loss: 2078456.12500\n",
      "Step #100, epoch #100, avg. train loss: 2209197.00000\n",
      "Step #100, epoch #100, avg. train loss: 3935593.25000\n",
      "Step #100, epoch #100, avg. train loss: 3600800.75000\n",
      "Step #100, epoch #100, avg. train loss: 3803316.25000\n",
      "Step #100, epoch #100, avg. train loss: 2367294.25000\n",
      "Step #100, epoch #100, avg. train loss: 2139623.75000\n",
      "Step #100, epoch #100, avg. train loss: 2267168.75000\n",
      "Step #100, epoch #100, avg. train loss: 2178367.00000\n",
      "Step #100, epoch #100, avg. train loss: 1952476.62500\n",
      "Step #100, epoch #100, avg. train loss: 2053988.00000\n",
      "Step #100, epoch #100, avg. train loss: 2925592.25000\n",
      "Step #100, epoch #100, avg. train loss: 2669351.25000\n",
      "Step #100, epoch #100, avg. train loss: 2820779.75000\n",
      "Step #100, epoch #100, avg. train loss: 2950780.50000\n",
      "Step #100, epoch #100, avg. train loss: 2775877.75000\n",
      "Step #100, epoch #100, avg. train loss: 3034507.25000\n",
      "Step #100, epoch #100, avg. train loss: 2444796.25000\n",
      "Step #100, epoch #100, avg. train loss: 2221740.25000\n",
      "Step #100, epoch #100, avg. train loss: 2352211.50000\n",
      "Step #100, epoch #100, avg. train loss: 2174454.50000\n",
      "Step #100, epoch #100, avg. train loss: 1951294.12500\n",
      "Step #100, epoch #100, avg. train loss: 2076313.00000\n",
      "Step #100, epoch #100, avg. train loss: 3109791.00000\n",
      "Step #100, epoch #100, avg. train loss: 2801481.00000\n",
      "Step #100, epoch #100, avg. train loss: 2876316.25000\n",
      "Step #100, epoch #100, avg. train loss: 2589545.75000\n",
      "Step #100, epoch #100, avg. train loss: 3138917.75000\n",
      "Step #100, epoch #100, avg. train loss: 2456370.50000\n",
      "Step #100, epoch #50, avg. train loss: 2884475.25000\n",
      "best CV score from grid search: 0.215183\n",
      "corresponding parameters: {'steps': 100, 'learning_rate': 1.0, 'hidden_units': [14, 14], 'batch_size': 300}\n",
      "Score: -0.089491\n"
     ]
    }
   ],
   "source": [
    "# DNN-Regressor tuned with GS\n",
    "\n",
    "# param_grid\n",
    "param_grid = {'hidden_units': [[11,11], [12,12], [13,13], [14,14], [15,15]], \n",
    "              'steps': [100],\n",
    "              'learning_rate': [0.1, 0.3, 0.7, 1.0],\n",
    "              'batch_size': [250, 300, 350, 400, 450]\n",
    "             }\n",
    "\n",
    "# GS with MSE\n",
    "#regressor_tuned = GridSearchCV(skflow.TensorFlowDNNRegressor (hidden_units = [10, 10]), param_grid, scoring = 'mean_squared_error')\n",
    "\n",
    "# GS with R^2\n",
    "regressor_tuned_GS = GridSearchCV(skflow.TensorFlowDNNRegressor (hidden_units = [10, 10]), param_grid, scoring = 'r2')\n",
    "\n",
    "# Fit\n",
    "regressor_tuned_GS.fit(X_train, y_train)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(regressor_tuned_GS.best_score_))\n",
    "print('corresponding parameters: {}'.format(regressor_tuned_GS.best_params_))\n",
    "\n",
    "# source: https://github.com/tensorflow/skflow/pull/126/files\n",
    "\n",
    "# Predict and score\n",
    "predict = regressor_tuned_GS.predict(X_test)\n",
    "\n",
    "#score_regressor_tuned_GS = mean_squared_error(y_test, predict)\n",
    "score_regressor_tuned_GS = r2_score(y_test, predict)\n",
    "\n",
    "print('Score: {0:f}'.format(score_regressor_tuned_GS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Regressor Results\n",
      "\n",
      "DNN: -0.488562\n",
      "DNN tuned grid: -0.089491\n"
     ]
    }
   ],
   "source": [
    "print('DNN Regressor Results\\n')\n",
    "print(\"DNN: %f\" % score_regressor)\n",
    "print(\"DNN tuned grid: %f\" % score_regressor_tuned_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100, epoch #50, avg. train loss: 2206362.25000\n",
      "Step #200, epoch #100, avg. train loss: 1672687.25000\n",
      "Step #100, epoch #50, avg. train loss: 1943511.00000\n",
      "Step #200, epoch #100, avg. train loss: 1346688.75000\n",
      "Step #100, epoch #50, avg. train loss: 2044458.50000\n",
      "Step #200, epoch #100, avg. train loss: 1440791.00000\n",
      "Step #100, epoch #50, avg. train loss: 2033024.50000\n",
      "Step #200, epoch #100, avg. train loss: 1477876.62500\n",
      "\n",
      " best CV score from grid search: 0.241827\n",
      "\n",
      " corresponding parameters: {'learning_rate': 0.35683995456013873, 'hidden_units': [12, 12], 'batch_size': 306}\n",
      "\n",
      " Score: 0.228429\n"
     ]
    }
   ],
   "source": [
    "# DNN-Regressor tuned with RandomizesSearch\n",
    "\n",
    "# Parameters\n",
    "param_dist = {  'hidden_units': [[11,11], [12,12], [13,13]], \n",
    "                'learning_rate': sp_uniform(0.0,1.0), \n",
    "                'batch_size': sp_randint(250, 350)\n",
    "             }\n",
    "\n",
    "n_iter_search = 1\n",
    "\n",
    "# MSE optimized\n",
    "#regressor_tuned_RS = RandomizedSearchCV(skflow.TensorFlowDNNRegressor (hidden_units = [10, 10]), param_distributions = param_dist, scoring = 'mean_squared_error', n_iter=n_iter_search)\n",
    "\n",
    "# R^2 optimized\n",
    "regressor_tuned_RS = RandomizedSearchCV(skflow.TensorFlowDNNRegressor (hidden_units = [10, 10]), param_distributions = param_dist, scoring = 'r2', n_iter=n_iter_search)\n",
    "\n",
    "# Fit\n",
    "regressor_tuned_RS.fit(X_train, y_train)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('\\n best CV score from grid search: {0:f}'.format(regressor_tuned_RS.best_score_))\n",
    "print('\\n corresponding parameters: {}'.format(regressor_tuned_RS.best_params_))\n",
    "\n",
    "# source: https://github.com/tensorflow/skflow/pull/126/files\n",
    "\n",
    "# Predict and score\n",
    "predict = regressor_tuned_RS.predict(X_test)\n",
    "\n",
    "#score_regressor_tuned_RS = mean_squared_error(y_test, predict)\n",
    "score_regressor_tuned_RS = r2_score(y_test, predict)\n",
    "\n",
    "print('\\n Score: {0:f}'.format(score_regressor_tuned_RS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same picture with the DNN Regressor. The tuning helps, but the results are still underwhelming. Also, the best DNN result is no match for the tuned SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "\n",
      "SVR: 0.001379\n",
      "SVR tuned grid: 0.778184\n",
      "SVR tuned random: 0.775381\n",
      "\n",
      "\n",
      "DNN: -0.488562\n",
      "DNN tuned grid: -0.089491\n",
      "DNN tuned random: 0.228429\n"
     ]
    }
   ],
   "source": [
    "print('Results\\n')\n",
    "\n",
    "print(\"SVR: %f\" % score_svr)\n",
    "print(\"SVR tuned grid: %f\" % score_svr_tuned_GS)\n",
    "print(\"SVR tuned random: %f\" % score_svr_tuned_RS)\n",
    "\n",
    "print('\\n')\n",
    "print(\"DNN: %f\" % score_regressor)\n",
    "print(\"DNN tuned grid: %f\" % score_regressor_tuned_GS)\n",
    "print(\"DNN tuned random: %f\" % score_regressor_tuned_RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR works better than the DNN Regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement  \n",
    "The count of rented bikes (cnt) is just the sum of the features casual and registered. Two separate models are trained to predict these features. And add up afterward. This split should improve the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns:\n",
      "Index([u'instant', u'dteday', u'season', u'yr', u'mnth', u'holiday',\n",
      "       u'weekday', u'workingday', u'weathersit', u'temp', u'atemp', u'hum',\n",
      "       u'windspeed'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column:\n",
      "casual\n",
      "\n",
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid=[{'kernel': ['linear', 'rbf'], 'C': [1, 3, 10, 30, 100, 300, 1000, 3000]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)\n",
      "\n",
      "Best parameter from grid search: {'kernel': 'linear', 'C': 1000}\n"
     ]
    }
   ],
   "source": [
    "#SVR with GridSearch - for casual users\n",
    "\n",
    "# Extracting\n",
    "feature_cols_cas = bike_data.columns[:-3]  # all columns but last are features\n",
    "target_col_cas = bike_data.columns[-3]  # last column is the target\n",
    "print (\"Feature columns:\\n{}\\n\".format(feature_cols_cas))\n",
    "print (\"Target column:\\n{}\\n\".format(target_col_cas))\n",
    "\n",
    "# Pre-processing\n",
    "X_cas = bike_data[feature_cols_cas.drop(['dteday'],['instant'])]  # feature values \n",
    "y_cas = bike_data[target_col_cas]  # corresponding targets\n",
    "\n",
    "# Split Set\n",
    "X_train_cas, X_test_cas, y_train_cas, y_test_cas = train_test_split(X_cas, y_cas)# test size is set to 0.25\n",
    "\n",
    "# Tuning SVR\n",
    "param_grid = [\n",
    "             {'C': [1, 3, 10, 30, 100, 300, 1000, 3000],\n",
    "              'kernel': ['linear', 'rbf']}\n",
    "             ]\n",
    "\n",
    "# MSR optimized\n",
    "#svr_tuned_cas = GridSearchCV(SVR (C=1), param_grid = param_grid, scoring = 'mean_squared_error')\n",
    "\n",
    "# R^2 optimized\n",
    "svr_tuned_cas_GS = GridSearchCV(SVR (C=1), param_grid = param_grid, scoring = 'r2', n_jobs=-1)\n",
    "\n",
    "# Fitting\n",
    "svr_tuned_cas_GS.fit(X_train_cas, y_train_cas)\n",
    "\n",
    "print (svr_tuned_cas_GS)\n",
    "print ('\\n' \"Best parameter from grid search: {}\".format(svr_tuned_cas_GS.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best CV score from grid search: 0.642586\n",
      "corresponding parameters: {'kernel': 'linear', 'C': 673.1319062838915}\n"
     ]
    }
   ],
   "source": [
    "# SVR with RandomizesSearch - for casual users\n",
    "# may take a while!\n",
    "\n",
    "# Parameters\n",
    "param_dist = {  'C': sp_uniform (300, 3000), \n",
    "                'kernel': ['linear']\n",
    "             }\n",
    "\n",
    "n_iter_search = 1\n",
    "\n",
    "svr_tuned_cas_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'r2', n_iter=n_iter_search)\n",
    "\n",
    "# Fit\n",
    "svr_tuned_cas_RS.fit(X_train_cas, y_train_cas)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(svr_tuned_cas_RS.best_score_))\n",
    "print('corresponding parameters: {}'.format(svr_tuned_cas_RS.best_params_))\n",
    "\n",
    "# Predict and score\n",
    "predict = svr_tuned_cas_RS.predict(X_test)\n",
    "\n",
    "#score_regressor_tuned_RS = mean_squared_error(y_test, predict)\n",
    "score_SVR_tuned_RS = r2_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):\n",
      "Index([u'instant', u'dteday', u'season', u'yr', u'mnth', u'holiday',\n",
      "       u'weekday', u'workingday', u'weathersit', u'temp', u'atemp', u'hum',\n",
      "       u'windspeed'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column:\n",
      "registered\n",
      "\n",
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid=[{'kernel': ['linear', 'rbf'], 'C': [1000, 3000, 10000]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)\n",
      "\n",
      "Best parameter from grid search:{'kernel': 'linear', 'C': 3000}\n"
     ]
    }
   ],
   "source": [
    "#SVR for casual with with GridSearch - for registered users\n",
    "\n",
    "# Extracting\n",
    "feature_cols_reg = bike_data.columns[:-3]  # all columns but last are features\n",
    "target_col_reg = bike_data.columns[-2]  # last column is the target\n",
    "print (\"Feature column(s):\\n{}\\n\".format(feature_cols_reg))\n",
    "print (\"Target column:\\n{}\\n\".format(target_col_reg))\n",
    "\n",
    "# Pre-processing\n",
    "X_reg = bike_data[feature_cols_reg.drop(['dteday'],['casual'])]  # feature values \n",
    "y_reg = bike_data[target_col_reg]  # corresponding targets\n",
    "\n",
    "# Split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg)# test size is set to 0.25\n",
    "\n",
    "# Tuning SVR\n",
    "param_grid = [\n",
    "             {'C': [1000, 3000, 10000],\n",
    "              'kernel': ['linear', 'rbf']}\n",
    "             ]\n",
    "\n",
    "#svr_tuned_reg = GridSearchCV(SVR (C=1), param_grid = param_grid, scoring = 'mean_squared_error')\n",
    "svr_tuned_reg_GS = GridSearchCV(SVR (C=1), param_grid = param_grid, scoring = 'r2', n_jobs=-1)\n",
    "\n",
    "\n",
    "# Fitting \n",
    "svr_tuned_reg_GS.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "print (svr_tuned_reg_GS)\n",
    "print ('\\n' \"Best parameter from grid search:{}\".format(svr_tuned_reg_GS.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best CV score from grid search: 0.642586\n",
      "corresponding parameters: {'kernel': 'linear', 'C': 673.1319062838915}\n"
     ]
    }
   ],
   "source": [
    "#SVR with RandomizesSearch - for registered users\n",
    "# may take a while!\n",
    "\n",
    "# Parameters\n",
    "param_dist = {  'C': sp_uniform (300, 3000), \n",
    "                'kernel': ['linear']\n",
    "             }\n",
    "\n",
    "n_iter_search = 1\n",
    "\n",
    "svr_tuned_reg_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'r2', n_iter=n_iter_search)\n",
    "\n",
    "# Fit\n",
    "svr_tuned_reg_RS.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(svr_tuned_cas_RS.best_score_))\n",
    "print('corresponding parameters: {}'.format(svr_tuned_cas_RS.best_params_))\n",
    "\n",
    "# Predict and score\n",
    "predict = svr_tuned_reg_RS.predict(X_test)\n",
    "\n",
    "#score_regressor_tuned_RS = mean_squared_error(y_test, predict)\n",
    "score_SVR_tuned_reg_RS = r2_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score cas: 0.660214\n",
      "Score reg: 0.813634\n",
      "Score sum: 0.787936\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "\n",
    "#print ('Score cas: {0:f}'.format(mean_squared_error(y_test_cas,svr_tuned_cas.predict(X_test_cas))))\n",
    "#print ('Score reg: {0:f}'.format(mean_squared_error(y_test_reg,svr_tuned_reg.predict(X_test_reg))))\n",
    "print ('Score cas: {0:f}'.format(r2_score(y_test_cas,svr_tuned_cas_RS.predict(X_test_cas))))\n",
    "print ('Score reg: {0:f}'.format(r2_score(y_test_reg,svr_tuned_reg_RS.predict(X_test_reg))))\n",
    "\n",
    "predict_sum_test = svr_tuned_cas_RS.predict(X_test) + svr_tuned_reg_RS.predict(X_test)\n",
    "\n",
    "#score = mean_squared_error(y_test, predict_sum)\n",
    "score = r2_score(y_test, predict_sum_test)\n",
    "\n",
    "print('Score sum: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR: 0.001379\n",
      "SVR tuned grid: 0.778184\n",
      "SVR tuned RS: 0.775381\n",
      "\n",
      "\n",
      "DNN: -0.488562\n",
      "DNN tuned grid: -0.089491\n",
      "DNN tuned random: 0.228429\n",
      "\n",
      "\n",
      "SVR sum: 0.787936\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(\"SVR: %f\" % score_svr)\n",
    "print(\"SVR tuned grid: %f\" % score_svr_tuned_GS)\n",
    "print(\"SVR tuned RS: %f\" % score_svr_tuned_RS)\n",
    "print('\\n')\n",
    "print(\"DNN: %f\" % score_regressor)\n",
    "print(\"DNN tuned grid: %f\" % score_regressor_tuned_GS)\n",
    "print(\"DNN tuned random: %f\" % score_regressor_tuned_RS)\n",
    "print('\\n')\n",
    "print('SVR sum: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The SVR beats the DNN Regressor by far.  \n",
    "- The separate prediction of casual and registered customers increases the R^2 slightly.  \n",
    "- More than 80% determination is a decent result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free-Form Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visulazation\n",
    "\n",
    "predict_sum_all = svr_tuned_cas_RS.predict(X_test) + svr_tuned_reg_RS.predict(X_test)\n",
    "predictions = pd.Series(predict_sum_all, index = y_test.index.values)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(1)\n",
    "\n",
    "plt.plot(y_test,'go', label='truth')\n",
    "plt.plot(predictions,'bx', label='prediction')\n",
    "\n",
    "plt.title('Number of bikes rented per day')\n",
    "plt.xlabel('Days')\n",
    "plt.xticks((np.arange(0,len(bike_data),len(bike_data)/24)), calendar.month_name[1:13]*2, rotation=45)\n",
    "\n",
    "plt.ylabel('Number of bikes')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# source: http://matplotlib.org/examples/showcase/bachelors_degrees_by_gender.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are reasonably good, without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I had high hopes for the DNN Regressor. It was kind of disappointing that it does not even come close. Maybe my tuning was not right or it needs more data or computational power.\n",
    "- Utilizing grid and randomize search in a way that makes sense was a little tricky. It makes more sense to start with a broad grid search and than use randomized search on the given interval, instead of vis a versa. It is also computational more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The determination could be increased by additional iterations in training and the number of folds in the cross-validation, at the expense of computing time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
