{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capstone Project\n",
    "Machine Learning Engineer Nanodegree\n",
    "\n",
    "# Demand prediction for a bike sharing systems\n",
    "\n",
    "Mai 2016  \n",
    "Philipp Vogler  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview  \n",
    "\n",
    "- Utelizing machine learning to forecast the demand for the washington DC bike sharing system 'capital bike share'  \n",
    "- Using different types of regression to find an algorithem to predict the demand for bikes based on calenderic and weather information.  \n",
    "- Weather, calendaric and demand information is provided in a dataset by the University of Porto at UCI ML Repository.  \n",
    "- This project tries to create a forecasting function based on two years of historic data by utelizing the machine learning libraries scikit-learn and tensor-flow.  \n",
    "\n",
    "> http://www.capitalbikeshare.com   \n",
    "> http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset  \n",
    "> http://freemeteo.de/wetter/  \n",
    "> http://dchr.dc.gov/page/holiday-schedules  \n",
    "> http://scikit-learn.org/stable/  \n",
    "> https://www.tensorflow.org  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement  \n",
    "\n",
    "The goal is to forecast the demand of bikes in in dependency of weather conditions like outside temperature and calendaric informations e.g. holidays. These information and the demand structure is provided in a set with two years of dayly historic data.  \n",
    "The demand is given as the total dayly demand and as a split for registered users and casual users. To increase the quality of the prediction registered user demand and casual user demand will be predicted seperatly in step two.  \n",
    "To make predictions machnie learning is used to train regressors. Scikit-Learn recomands a support vector regressor (SVR) for this kind of problem and dataset. In addition a deep neuronal network (DNN) regressor is trained for comparison. To find the hyperparameters for these regressors grid search and ramdomized search are utelized. Due to the small dataset cross validation is applied.  \n",
    "\n",
    "> http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR  \n",
    "> https://github.com/tensorflow/skflow/blob/master/g3doc/api_docs/python/estimators.md  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "To mesure the performance of the regressions two standard regression metrics are used: Mean squared eror (MSE) and the coefficient of determination (R^2). Both metrics are calculated for both regressor types. For comparison and parameter tuneing only R^2 is used due to the better readability.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from tensorflow.contrib import skflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Getting Dataset\n",
    "\n",
    "bike_data = pd.read_csv(\"day.csv\")\n",
    "\n",
    "print \"Data read successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):\n",
      "Index([u'instant', u'dteday', u'season', u'yr', u'mnth', u'holiday',\n",
      "       u'weekday', u'workingday', u'weathersit', u'temp', u'atemp', u'hum',\n",
      "       u'windspeed'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column:\n",
      "cnt\n"
     ]
    }
   ],
   "source": [
    "# Extracting\n",
    "\n",
    "feature_cols = bike_data.columns[:-3]  # all columns but last are features\n",
    "target_col = bike_data.columns[-1]  # last column is the target\n",
    "print (\"Feature column(s):\\n{}\\n\".format(feature_cols))\n",
    "print (\"Target column:\\n{}\".format(target_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Data values:\n",
      "   instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01       1   0     1        0        6           0   \n",
      "1        2  2011-01-02       1   0     1        0        0           0   \n",
      "2        3  2011-01-03       1   0     1        0        1           1   \n",
      "3        4  2011-01-04       1   0     1        0        2           1   \n",
      "4        5  2011-01-05       1   0     1        0        3           1   \n",
      "\n",
      "   weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
      "0           2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
      "1           2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
      "2           1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
      "3           1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
      "4           1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
      "\n",
      "    cnt  \n",
      "0   985  \n",
      "1   801  \n",
      "2  1349  \n",
      "3  1562  \n",
      "4  1600  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>366.000000</td>\n",
       "      <td>2.496580</td>\n",
       "      <td>0.500684</td>\n",
       "      <td>6.519836</td>\n",
       "      <td>0.028728</td>\n",
       "      <td>2.997264</td>\n",
       "      <td>0.683995</td>\n",
       "      <td>1.395349</td>\n",
       "      <td>0.495385</td>\n",
       "      <td>0.474354</td>\n",
       "      <td>0.627894</td>\n",
       "      <td>0.190486</td>\n",
       "      <td>848.176471</td>\n",
       "      <td>3656.172367</td>\n",
       "      <td>4504.348837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>211.165812</td>\n",
       "      <td>1.110807</td>\n",
       "      <td>0.500342</td>\n",
       "      <td>3.451913</td>\n",
       "      <td>0.167155</td>\n",
       "      <td>2.004787</td>\n",
       "      <td>0.465233</td>\n",
       "      <td>0.544894</td>\n",
       "      <td>0.183051</td>\n",
       "      <td>0.162961</td>\n",
       "      <td>0.142429</td>\n",
       "      <td>0.077498</td>\n",
       "      <td>686.622488</td>\n",
       "      <td>1560.256377</td>\n",
       "      <td>1937.211452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059130</td>\n",
       "      <td>0.079070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022392</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>183.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337083</td>\n",
       "      <td>0.337842</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.134950</td>\n",
       "      <td>315.500000</td>\n",
       "      <td>2497.000000</td>\n",
       "      <td>3152.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>366.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498333</td>\n",
       "      <td>0.486733</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.180975</td>\n",
       "      <td>713.000000</td>\n",
       "      <td>3662.000000</td>\n",
       "      <td>4548.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>548.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.655417</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>0.730209</td>\n",
       "      <td>0.233214</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>4776.500000</td>\n",
       "      <td>5956.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.861667</td>\n",
       "      <td>0.840896</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>3410.000000</td>\n",
       "      <td>6946.000000</td>\n",
       "      <td>8714.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          instant      season          yr        mnth     holiday     weekday  \\\n",
       "count  731.000000  731.000000  731.000000  731.000000  731.000000  731.000000   \n",
       "mean   366.000000    2.496580    0.500684    6.519836    0.028728    2.997264   \n",
       "std    211.165812    1.110807    0.500342    3.451913    0.167155    2.004787   \n",
       "min      1.000000    1.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "25%    183.500000    2.000000    0.000000    4.000000    0.000000    1.000000   \n",
       "50%    366.000000    3.000000    1.000000    7.000000    0.000000    3.000000   \n",
       "75%    548.500000    3.000000    1.000000   10.000000    0.000000    5.000000   \n",
       "max    731.000000    4.000000    1.000000   12.000000    1.000000    6.000000   \n",
       "\n",
       "       workingday  weathersit        temp       atemp         hum   windspeed  \\\n",
       "count  731.000000  731.000000  731.000000  731.000000  731.000000  731.000000   \n",
       "mean     0.683995    1.395349    0.495385    0.474354    0.627894    0.190486   \n",
       "std      0.465233    0.544894    0.183051    0.162961    0.142429    0.077498   \n",
       "min      0.000000    1.000000    0.059130    0.079070    0.000000    0.022392   \n",
       "25%      0.000000    1.000000    0.337083    0.337842    0.520000    0.134950   \n",
       "50%      1.000000    1.000000    0.498333    0.486733    0.626667    0.180975   \n",
       "75%      1.000000    2.000000    0.655417    0.608602    0.730209    0.233214   \n",
       "max      1.000000    3.000000    0.861667    0.840896    0.972500    0.507463   \n",
       "\n",
       "            casual   registered          cnt  \n",
       "count   731.000000   731.000000   731.000000  \n",
       "mean    848.176471  3656.172367  4504.348837  \n",
       "std     686.622488  1560.256377  1937.211452  \n",
       "min       2.000000    20.000000    22.000000  \n",
       "25%     315.500000  2497.000000  3152.000000  \n",
       "50%     713.000000  3662.000000  4548.000000  \n",
       "75%    1096.000000  4776.500000  5956.000000  \n",
       "max    3410.000000  6946.000000  8714.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploration\n",
    "\n",
    "print \"\\n Data values:\"\n",
    "print bike_data.head()  # print the first 5 rows\n",
    "\n",
    "bike_data.describe() # shows stats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics\n",
    "\n",
    "Most of the data is already normalized or binary.  \n",
    "The dataset is very concise and missing values are not a problem. Categorical data like weekday or workingday are already processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing (Methodology)\n",
    "\n",
    "Dates get droped because the regressor can not read this datatype and the order information is already stored in the index. The instant variable replicates this information also. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "\n",
    "X = bike_data[feature_cols.drop(['dteday'],['instant'])] # feature values \n",
    "y = bike_data[target_col]  # corresponding targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "\n",
    "The visualiation shows a classic seasonal pattern with a up trend year over year. There are so outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visulazation\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(1)\n",
    "      \n",
    "plt.plot(bike_data.cnt,'bo')\n",
    "\n",
    "plt.title('Number of bikes rented per day')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number of bikes')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# source: http://matplotlib.org/examples/showcase/bachelors_degrees_by_gender.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)# test size is set to 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of regressors are trained. A SVR and a DNN-Regressor. Both are first used \"of the shelf\" with default parameters to create a benchmark.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR  \n",
    "> https://github.com/tensorflow/skflow/blob/master/g3doc/api_docs/python/estimators.md  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training SVR\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score SVR: -0.001130\n"
     ]
    }
   ],
   "source": [
    "# Validation SVR\n",
    "\n",
    "svr_pred = svr.predict(X_test)\n",
    "\n",
    "# score_svr = mean_squared_error(y_test, svr_pred)\n",
    "score_svr = r2_score(y_test, svr_pred)\n",
    "\n",
    "print(\"Score SVR: %f\" % score_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #99, avg. train loss: 3207103.00000\n",
      "Step #199, avg. train loss: 1926467.00000\n",
      "Step #299, avg. train loss: 2688508.75000\n",
      "Step #399, avg. train loss: 2056654.50000\n",
      "Step #499, avg. train loss: 2497213.50000\n",
      "Step #600, epoch #1, avg. train loss: 2368136.75000\n",
      "Step #700, epoch #1, avg. train loss: 2958193.00000\n",
      "Step #800, epoch #1, avg. train loss: 2027419.50000\n",
      "Step #900, epoch #1, avg. train loss: 2673544.75000\n",
      "Step #1000, epoch #1, avg. train loss: 1638436.00000\n",
      "Step #1100, epoch #2, avg. train loss: 1573353.62500\n",
      "Step #1200, epoch #2, avg. train loss: 2222704.25000\n",
      "Step #1300, epoch #2, avg. train loss: 2296603.50000\n",
      "Step #1400, epoch #2, avg. train loss: 1403950.37500\n",
      "Step #1500, epoch #2, avg. train loss: 1412117.50000\n",
      "Step #1600, epoch #2, avg. train loss: 2502098.50000\n",
      "Step #1700, epoch #3, avg. train loss: 3401696.00000\n",
      "Step #1800, epoch #3, avg. train loss: 1784947.87500\n",
      "Step #1900, epoch #3, avg. train loss: 2403448.25000\n",
      "Step #2000, epoch #3, avg. train loss: 2726404.25000\n",
      "Step #2100, epoch #3, avg. train loss: 2041935.62500\n",
      "Step #2200, epoch #4, avg. train loss: 1967384.50000\n",
      "Step #2300, epoch #4, avg. train loss: 1863739.87500\n",
      "Step #2400, epoch #4, avg. train loss: 2013118.37500\n",
      "Step #2500, epoch #4, avg. train loss: 1513165.75000\n",
      "Step #2600, epoch #4, avg. train loss: 3048063.75000\n",
      "Step #2700, epoch #4, avg. train loss: 1739552.00000\n",
      "Step #2800, epoch #5, avg. train loss: 2240944.00000\n",
      "Step #2900, epoch #5, avg. train loss: 1405356.37500\n",
      "Step #3000, epoch #5, avg. train loss: 1615602.87500\n",
      "Step #3100, epoch #5, avg. train loss: 3120635.50000\n",
      "Step #3200, epoch #5, avg. train loss: 2531340.25000\n",
      "Step #3300, epoch #6, avg. train loss: 1635764.50000\n",
      "Step #3400, epoch #6, avg. train loss: 2112303.75000\n",
      "Step #3500, epoch #6, avg. train loss: 1720380.62500\n",
      "Step #3600, epoch #6, avg. train loss: 1562997.00000\n",
      "Step #3700, epoch #6, avg. train loss: 2615837.25000\n",
      "Step #3800, epoch #6, avg. train loss: 1705417.87500\n",
      "Step #3900, epoch #7, avg. train loss: 2061492.75000\n",
      "Step #4000, epoch #7, avg. train loss: 2140562.75000\n",
      "Step #4100, epoch #7, avg. train loss: 2148273.75000\n",
      "Step #4200, epoch #7, avg. train loss: 1448758.87500\n",
      "Step #4300, epoch #7, avg. train loss: 1815587.50000\n",
      "Step #4400, epoch #8, avg. train loss: 1865443.37500\n",
      "Step #4500, epoch #8, avg. train loss: 2787892.50000\n",
      "Step #4600, epoch #8, avg. train loss: 2117357.50000\n",
      "Step #4700, epoch #8, avg. train loss: 1286700.12500\n",
      "Step #4800, epoch #8, avg. train loss: 1858173.62500\n",
      "Step #4900, epoch #8, avg. train loss: 2032580.75000\n",
      "Step #5000, epoch #9, avg. train loss: 1549823.00000\n",
      "Score: -0.463030\n"
     ]
    }
   ],
   "source": [
    "# DNN-Regressor\n",
    "\n",
    "#  Copyright 2015-present The Scikit Flow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\n",
    "#from __future__ import absolute_import\n",
    "#from __future__ import division\n",
    "#from __future__ import print_function\n",
    "#from sklearn import preprocessing\n",
    "\n",
    "# Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
    "regressor = skflow.TensorFlowDNNRegressor(hidden_units=[10,10], steps=5000, learning_rate=0.1, batch_size=1)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and validate\n",
    "#score_regressor = metrics.mean_squared_error( y_test, regressor.predict(X_test))\n",
    "score_regressor = r2_score(y_test, regressor.predict(X_test))\n",
    "\n",
    "print('Score: {0:f}'.format(score_regressor))\n",
    "\n",
    "# source https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/boston.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both \"bench marks\" for the coefficient of determination are very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Afterwards they are trained using randomised search and cross validation to the area of the best parameters. Last a grid search is used to tune parameter values.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randomized search is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid=[{'kernel': ['linear', 'rbf'], 'C': [0.0001, 0.001, 0.01]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)\n",
      "\n",
      "Best parameter from grid search: {'kernel': 'linear', 'C': 0.001}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tuning SVR\n",
    "\n",
    "tuned_parameters = [{'C': [0.0001, 0.001, 0.01], \n",
    "                     'kernel': ['linear', 'rbf']}\n",
    "                   ]\n",
    "\n",
    "#svr_tuned = GridSearchCV(SVR (C=1), param_grid = tuned_parameters, scoring = 'mean_squared_error') #default 3-fold cross-validation, score method of the estimator\n",
    "svr_tuned = GridSearchCV(SVR (C=1), param_grid = tuned_parameters, scoring = 'r2') #default 3-fold cross-validation, score method of the estimator\n",
    "\n",
    "svr_tuned.fit(X_train, y_train)\n",
    "\n",
    "print (svr_tuned)\n",
    "print ('\\n' \"Best parameter from grid search: \" + str(svr_tuned.best_params_) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score SVR tuned: 0.245852\n",
      "Score SVR: -0.001130\n"
     ]
    }
   ],
   "source": [
    "# Validation - SVR tuned \n",
    "\n",
    "svr_tuned_pred = svr_tuned.predict(X_test)\n",
    "\n",
    "#score_svr_tuned = mean_squared_error(y_test, svr_tuned_pred)\n",
    "score_svr_tuned = r2_score(y_test, svr_tuned_pred)\n",
    "\n",
    "print(\"Score SVR tuned: %f\" % score_svr_tuned)\n",
    "print(\"Score SVR: %f\" % score_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuneing works but a coeficient of determination of about 25% is still not very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100, epoch #50, avg. train loss: 3118147.00000\n",
      "Step #200, epoch #100, avg. train loss: 1675743.25000\n",
      "Step #100, epoch #50, avg. train loss: 3013337.25000\n",
      "Step #200, epoch #100, avg. train loss: 1503629.25000\n",
      "Step #100, epoch #50, avg. train loss: 2835811.25000\n",
      "Step #200, epoch #100, avg. train loss: 1512915.87500\n",
      "Step #100, epoch #20, avg. train loss: 2696557.00000\n",
      "Step #200, epoch #40, avg. train loss: 1589748.37500\n",
      "Step #100, epoch #20, avg. train loss: 2602746.00000\n",
      "Step #200, epoch #40, avg. train loss: 1532425.00000\n",
      "Step #100, epoch #20, avg. train loss: 2579098.25000\n",
      "Step #200, epoch #40, avg. train loss: 1493449.00000\n",
      "Step #100, epoch #100, avg. train loss: 2874898.00000\n",
      "Step #200, epoch #200, avg. train loss: 1566988.37500\n",
      "Step #100, epoch #100, avg. train loss: 2795937.50000\n",
      "Step #200, epoch #200, avg. train loss: 1457326.25000\n",
      "Step #100, epoch #100, avg. train loss: 2862808.75000\n",
      "Step #200, epoch #200, avg. train loss: 1517723.87500\n",
      "Step #100, epoch #100, avg. train loss: 2121054.75000\n",
      "Step #200, epoch #200, avg. train loss: 1533969.62500\n",
      "Step #100, epoch #100, avg. train loss: 2023254.50000\n",
      "Step #200, epoch #200, avg. train loss: 1473864.37500\n",
      "Step #100, epoch #100, avg. train loss: 1990704.00000\n",
      "Step #200, epoch #200, avg. train loss: 1482430.37500\n",
      "Step #100, epoch #100, avg. train loss: 2239563.00000\n",
      "Step #200, epoch #200, avg. train loss: 1715007.25000\n",
      "Step #100, epoch #100, avg. train loss: 2150728.75000\n",
      "Step #200, epoch #200, avg. train loss: 1624080.37500\n",
      "Step #100, epoch #100, avg. train loss: 2113555.00000\n",
      "Step #200, epoch #200, avg. train loss: 1601673.12500\n",
      "Step #100, epoch #12, avg. train loss: 3338218.25000\n",
      "Step #200, epoch #25, avg. train loss: 1533279.62500\n",
      "Step #100, epoch #12, avg. train loss: 2879434.50000\n",
      "Step #200, epoch #25, avg. train loss: 1538001.62500\n",
      "Step #100, epoch #12, avg. train loss: 3081834.50000\n",
      "Step #200, epoch #25, avg. train loss: 1373247.25000\n",
      "Step #100, epoch #33, avg. train loss: 2217687.75000\n",
      "Step #200, epoch #66, avg. train loss: 1558581.12500\n",
      "Step #100, epoch #33, avg. train loss: 2178763.25000\n",
      "Step #200, epoch #66, avg. train loss: 1496882.12500\n",
      "Step #100, epoch #33, avg. train loss: 2138318.75000\n",
      "Step #200, epoch #66, avg. train loss: 1457136.50000\n",
      "Step #100, epoch #25, avg. train loss: 2189425.50000\n",
      "Step #200, epoch #50, avg. train loss: 1713870.87500\n",
      "Step #100, epoch #25, avg. train loss: 2071224.00000\n",
      "Step #200, epoch #50, avg. train loss: 1570226.25000\n",
      "Step #100, epoch #25, avg. train loss: 2028883.25000\n",
      "Step #200, epoch #50, avg. train loss: 1587579.62500\n",
      "Step #100, epoch #100, avg. train loss: 3569109.75000\n",
      "Step #200, epoch #200, avg. train loss: 1817694.50000\n",
      "Step #100, epoch #100, avg. train loss: 3559744.75000\n",
      "Step #200, epoch #200, avg. train loss: 1728464.37500\n",
      "Step #100, epoch #100, avg. train loss: 3473185.00000\n",
      "Step #200, epoch #200, avg. train loss: 1684293.75000\n",
      "Step #100, epoch #100, avg. train loss: 2145414.50000\n",
      "Step #200, epoch #200, avg. train loss: 1552030.75000\n",
      "Step #100, epoch #100, avg. train loss: 2060963.37500\n",
      "Step #200, epoch #200, avg. train loss: 1448578.37500\n",
      "Step #100, epoch #100, avg. train loss: 2029101.25000\n",
      "Step #200, epoch #200, avg. train loss: 1489911.62500\n",
      "Step #100, epoch #100, avg. train loss: 2145878.25000\n",
      "Step #200, epoch #200, avg. train loss: 1564121.12500\n",
      "Step #100, epoch #100, avg. train loss: 2045748.37500\n",
      "Step #200, epoch #200, avg. train loss: 1524184.50000\n",
      "Step #100, epoch #100, avg. train loss: 2021602.75000\n",
      "Step #200, epoch #200, avg. train loss: 1486842.25000\n",
      "Step #100, epoch #100, avg. train loss: 2149551.25000\n",
      "Step #200, epoch #200, avg. train loss: 1660342.75000\n",
      "Step #100, epoch #100, avg. train loss: 2023823.37500\n",
      "Step #200, epoch #200, avg. train loss: 1443044.12500\n",
      "Step #100, epoch #100, avg. train loss: 1988940.12500\n",
      "Step #200, epoch #200, avg. train loss: 1407083.25000\n",
      "Step #100, epoch #100, avg. train loss: 2146495.25000\n",
      "Step #200, epoch #200, avg. train loss: 1488814.37500\n",
      "Step #100, epoch #100, avg. train loss: 2049432.75000\n",
      "Step #200, epoch #200, avg. train loss: 1453505.62500\n",
      "Step #100, epoch #100, avg. train loss: 2025229.12500\n",
      "Step #200, epoch #200, avg. train loss: 1459177.87500\n",
      "Step #100, epoch #100, avg. train loss: 6917519.50000\n",
      "Step #200, epoch #200, avg. train loss: 2103193.50000\n",
      "Step #100, epoch #100, avg. train loss: 6940638.00000\n",
      "Step #200, epoch #200, avg. train loss: 2051635.37500\n",
      "Step #100, epoch #100, avg. train loss: 6861337.50000\n",
      "Step #200, epoch #200, avg. train loss: 1989508.37500\n",
      "Step #100, epoch #100, avg. train loss: 2163964.25000\n",
      "Step #200, epoch #200, avg. train loss: 1549716.37500\n",
      "Step #100, epoch #100, avg. train loss: 2089732.62500\n",
      "Step #200, epoch #200, avg. train loss: 1483352.12500\n",
      "Step #100, epoch #100, avg. train loss: 2047401.62500\n",
      "Step #200, epoch #200, avg. train loss: 1481800.50000\n",
      "Step #100, epoch #50, avg. train loss: 2161657.00000\n",
      "Step #200, epoch #100, avg. train loss: 1613637.25000\n",
      "Step #100, epoch #50, avg. train loss: 2102109.00000\n",
      "Step #200, epoch #100, avg. train loss: 1496815.00000\n",
      "Step #100, epoch #50, avg. train loss: 2092452.37500\n",
      "Step #200, epoch #100, avg. train loss: 1510304.62500\n",
      "Step #100, epoch #100, avg. train loss: 2206145.00000\n",
      "Step #200, epoch #200, avg. train loss: 1687231.87500\n",
      "Step #100, epoch #100, avg. train loss: 2121655.25000\n",
      "Step #200, epoch #200, avg. train loss: 1568946.37500\n",
      "Step #100, epoch #100, avg. train loss: 2085930.12500\n",
      "Step #200, epoch #200, avg. train loss: 1584513.25000\n",
      "Step #100, epoch #50, avg. train loss: 2101959.00000\n",
      "Step #200, epoch #100, avg. train loss: 1679007.50000\n",
      "Step #100, epoch #50, avg. train loss: 2082761.25000\n",
      "Step #200, epoch #100, avg. train loss: 1564647.87500\n",
      "Step #100, epoch #50, avg. train loss: 2017006.75000\n",
      "Step #200, epoch #100, avg. train loss: 1585315.00000\n",
      "Step #100, epoch #100, avg. train loss: 2979496.75000\n",
      "Step #200, epoch #200, avg. train loss: 1620901.50000\n",
      "Step #100, epoch #100, avg. train loss: 2773140.75000\n",
      "Step #200, epoch #200, avg. train loss: 1487428.62500\n",
      "Step #100, epoch #100, avg. train loss: 2741989.00000\n",
      "Step #200, epoch #200, avg. train loss: 1519448.62500\n",
      "Step #100, epoch #100, avg. train loss: 2109053.00000\n",
      "Step #200, epoch #200, avg. train loss: 1621135.87500\n",
      "Step #100, epoch #100, avg. train loss: 2020967.00000\n",
      "Step #200, epoch #200, avg. train loss: 1496219.25000\n",
      "Step #100, epoch #100, avg. train loss: 1986484.37500\n",
      "Step #200, epoch #200, avg. train loss: 1512070.50000\n",
      "Step #100, epoch #100, avg. train loss: 2864416.00000\n",
      "Step #200, epoch #200, avg. train loss: 1460488.37500\n",
      "Step #100, epoch #100, avg. train loss: 2868313.00000\n",
      "Step #200, epoch #200, avg. train loss: 1558566.50000\n",
      "Step #100, epoch #100, avg. train loss: 2791763.75000\n",
      "Step #200, epoch #200, avg. train loss: 1429435.50000\n",
      "Step #100, epoch #100, avg. train loss: 2129424.50000\n",
      "Step #200, epoch #200, avg. train loss: 1461896.62500\n",
      "Step #100, epoch #100, avg. train loss: 2057821.87500\n",
      "Step #200, epoch #200, avg. train loss: 1474911.37500\n",
      "Step #100, epoch #100, avg. train loss: 2027808.75000\n",
      "Step #200, epoch #200, avg. train loss: 1434893.25000\n",
      "Step #100, epoch #100, avg. train loss: 2783234.50000\n",
      "Step #200, epoch #200, avg. train loss: 1476994.75000\n",
      "Step #100, epoch #100, avg. train loss: 2548313.25000\n",
      "Step #200, epoch #200, avg. train loss: 1710365.75000\n",
      "Step #100, epoch #100, avg. train loss: 2687563.50000\n",
      "Step #200, epoch #200, avg. train loss: 1466156.50000\n",
      "Step #100, epoch #100, avg. train loss: 2508136.00000\n",
      "Step #200, epoch #200, avg. train loss: 1760647.62500\n",
      "Step #100, epoch #100, avg. train loss: 2440484.25000\n",
      "Step #200, epoch #200, avg. train loss: 1675020.12500\n",
      "Step #100, epoch #100, avg. train loss: 2392014.00000\n",
      "Step #200, epoch #200, avg. train loss: 1641973.50000\n",
      "Step #100, epoch #50, avg. train loss: 2142770.00000\n",
      "Step #200, epoch #100, avg. train loss: 1758006.37500\n",
      "Step #100, epoch #50, avg. train loss: 2107051.75000\n",
      "Step #200, epoch #100, avg. train loss: 1614162.25000\n",
      "Step #100, epoch #50, avg. train loss: 2076192.37500\n",
      "Step #200, epoch #100, avg. train loss: 1585105.00000\n",
      "Step #100, epoch #100, avg. train loss: 2140222.75000\n",
      "Step #200, epoch #200, avg. train loss: 1688754.50000\n",
      "Step #100, epoch #100, avg. train loss: 2058280.75000\n",
      "Step #200, epoch #200, avg. train loss: 1544063.00000\n",
      "Step #100, epoch #100, avg. train loss: 2017600.50000\n",
      "Step #200, epoch #200, avg. train loss: 1457624.62500\n",
      "Step #100, epoch #100, avg. train loss: 2082327.25000\n",
      "Step #200, epoch #200, avg. train loss: 1537777.50000\n",
      "Step #100, epoch #100, avg. train loss: 1999807.00000\n",
      "Step #200, epoch #200, avg. train loss: 1516445.62500\n",
      "Step #100, epoch #100, avg. train loss: 1966889.75000\n",
      "Step #200, epoch #200, avg. train loss: 1502555.37500\n",
      "Step #100, epoch #100, avg. train loss: 2098873.75000\n",
      "Step #200, epoch #200, avg. train loss: 1665033.25000\n",
      "Step #100, epoch #100, avg. train loss: 1991685.12500\n",
      "Step #200, epoch #200, avg. train loss: 1486177.00000\n",
      "Step #100, epoch #100, avg. train loss: 1965582.87500\n",
      "Step #200, epoch #200, avg. train loss: 1500162.50000\n",
      "Step #100, epoch #7, avg. train loss: 2335616.50000\n",
      "Step #200, epoch #15, avg. train loss: 1702518.12500\n",
      "Step #100, epoch #7, avg. train loss: 2256218.00000\n",
      "Step #200, epoch #15, avg. train loss: 1584277.12500\n",
      "Step #100, epoch #7, avg. train loss: 2114357.50000\n",
      "Step #200, epoch #15, avg. train loss: 1527193.12500\n",
      "Step #100, epoch #100, avg. train loss: 2177895.00000\n",
      "Step #200, epoch #200, avg. train loss: 1501890.50000\n",
      "Step #100, epoch #100, avg. train loss: 2084004.62500\n",
      "Step #200, epoch #200, avg. train loss: 1457320.12500\n",
      "Step #100, epoch #100, avg. train loss: 2056685.12500\n",
      "Step #200, epoch #200, avg. train loss: 1463807.50000\n",
      "Step #100, epoch #100, avg. train loss: 2237286.50000\n",
      "Step #200, epoch #200, avg. train loss: 1470231.87500\n",
      "Step #100, epoch #100, avg. train loss: 2190460.25000\n",
      "Step #200, epoch #200, avg. train loss: 1507995.25000\n",
      "Step #100, epoch #100, avg. train loss: 2134469.00000\n",
      "Step #200, epoch #200, avg. train loss: 1425025.75000\n",
      "Step #100, epoch #50, avg. train loss: 2695234.00000\n",
      "Step #200, epoch #100, avg. train loss: 1875990.87500\n",
      "Step #100, epoch #50, avg. train loss: 2675983.00000\n",
      "Step #200, epoch #100, avg. train loss: 1721540.62500\n",
      "Step #100, epoch #50, avg. train loss: 2618878.75000\n",
      "Step #200, epoch #100, avg. train loss: 1698819.25000\n",
      "Step #100, epoch #50, avg. train loss: 3136448.25000\n",
      "Step #200, epoch #100, avg. train loss: 1497637.00000\n",
      "Step #100, epoch #50, avg. train loss: 3011859.50000\n",
      "Step #200, epoch #100, avg. train loss: 1622187.37500\n",
      "Step #100, epoch #50, avg. train loss: 3033484.50000\n",
      "Step #200, epoch #100, avg. train loss: 1487334.50000\n",
      "Step #100, epoch #100, avg. train loss: 2678718.50000\n",
      "Step #200, epoch #200, avg. train loss: 1521375.50000\n",
      "Step #100, epoch #100, avg. train loss: 2904657.25000\n",
      "Step #200, epoch #200, avg. train loss: 1507618.37500\n",
      "Step #100, epoch #100, avg. train loss: 2644280.75000\n",
      "Step #200, epoch #200, avg. train loss: 1610597.62500\n",
      "Step #100, epoch #100, avg. train loss: 2618071.00000\n",
      "Step #200, epoch #200, avg. train loss: 1645522.50000\n",
      "Step #100, epoch #100, avg. train loss: 2552855.25000\n",
      "Step #200, epoch #200, avg. train loss: 1586932.00000\n",
      "Step #100, epoch #100, avg. train loss: 2505681.00000\n",
      "Step #200, epoch #200, avg. train loss: 1549869.50000\n",
      "Step #100, epoch #100, avg. train loss: 2232187.00000\n",
      "Step #200, epoch #200, avg. train loss: 1516712.62500\n",
      "Step #100, epoch #100, avg. train loss: 2227623.00000\n",
      "Step #200, epoch #200, avg. train loss: 1494405.87500\n",
      "Step #100, epoch #100, avg. train loss: 2159121.00000\n",
      "Step #200, epoch #200, avg. train loss: 1487216.50000\n",
      "Step #100, epoch #100, avg. train loss: 2257004.75000\n",
      "Step #200, epoch #200, avg. train loss: 1584458.87500\n",
      "Step #100, epoch #100, avg. train loss: 2171579.25000\n",
      "Step #200, epoch #200, avg. train loss: 1537936.00000\n",
      "Step #100, epoch #100, avg. train loss: 2136305.25000\n",
      "Step #200, epoch #200, avg. train loss: 1519136.75000\n",
      "Step #100, epoch #100, avg. train loss: 2157437.00000\n",
      "Step #200, epoch #200, avg. train loss: 1639732.75000\n",
      "Step #100, epoch #100, avg. train loss: 2083291.25000\n",
      "Step #200, epoch #200, avg. train loss: 1597869.75000\n",
      "Step #100, epoch #100, avg. train loss: 2041288.37500\n",
      "Step #200, epoch #200, avg. train loss: 1559733.12500\n",
      "Step #100, epoch #11, avg. train loss: 2997151.25000\n",
      "Step #200, epoch #22, avg. train loss: 1696309.75000\n",
      "Step #100, epoch #11, avg. train loss: 2707479.75000\n",
      "Step #200, epoch #22, avg. train loss: 1657728.50000\n",
      "Step #100, epoch #11, avg. train loss: 2885359.00000\n",
      "Step #200, epoch #22, avg. train loss: 1533156.75000\n",
      "Step #100, epoch #100, avg. train loss: 2178130.75000\n",
      "Step #200, epoch #200, avg. train loss: 1645125.00000\n",
      "Step #100, epoch #100, avg. train loss: 2070534.50000\n",
      "Step #200, epoch #200, avg. train loss: 1450124.00000\n",
      "Step #100, epoch #100, avg. train loss: 2045857.12500\n",
      "Step #200, epoch #200, avg. train loss: 1474555.25000\n",
      "Step #100, epoch #100, avg. train loss: 2224553.50000\n",
      "Step #200, epoch #200, avg. train loss: 1479101.50000\n",
      "Step #100, epoch #100, avg. train loss: 2147131.50000\n",
      "Step #200, epoch #200, avg. train loss: 1518693.62500\n",
      "Step #100, epoch #100, avg. train loss: 2117104.25000\n",
      "Step #200, epoch #200, avg. train loss: 1455524.62500\n",
      "Step #100, epoch #100, avg. train loss: 2360225.50000\n",
      "Step #200, epoch #200, avg. train loss: 1557422.25000\n",
      "Step #100, epoch #100, avg. train loss: 2249021.75000\n",
      "Step #200, epoch #200, avg. train loss: 1488279.62500\n",
      "Step #100, epoch #100, avg. train loss: 2227379.50000\n",
      "Step #200, epoch #200, avg. train loss: 1502256.50000\n",
      "Step #100, epoch #100, avg. train loss: 2252727.75000\n",
      "Step #200, epoch #200, avg. train loss: 1505178.37500\n",
      "Step #100, epoch #100, avg. train loss: 2152641.25000\n",
      "Step #200, epoch #200, avg. train loss: 1453104.50000\n",
      "Step #100, epoch #100, avg. train loss: 2134554.50000\n",
      "Step #200, epoch #200, avg. train loss: 1457035.62500\n",
      "Step #100, epoch #100, avg. train loss: 2611517.75000\n",
      "Step #200, epoch #200, avg. train loss: 1772549.87500\n",
      "Step #100, epoch #100, avg. train loss: 2533528.75000\n",
      "Step #200, epoch #200, avg. train loss: 1688405.62500\n",
      "Step #100, epoch #100, avg. train loss: 2491981.50000\n",
      "Step #200, epoch #200, avg. train loss: 1657255.25000\n",
      "Step #100, epoch #50, avg. train loss: 2172801.75000\n",
      "Step #200, epoch #100, avg. train loss: 1850666.12500\n",
      "Step #100, epoch #50, avg. train loss: 2099707.00000\n",
      "Step #200, epoch #100, avg. train loss: 1499071.37500\n",
      "Step #100, epoch #50, avg. train loss: 2044603.37500\n",
      "Step #200, epoch #100, avg. train loss: 1699469.87500\n",
      "Step #100, epoch #20, avg. train loss: 2218158.75000\n",
      "Step #200, epoch #40, avg. train loss: 1708546.12500\n",
      "Step #100, epoch #20, avg. train loss: 2098019.00000\n",
      "Step #200, epoch #40, avg. train loss: 1613126.37500\n",
      "Step #100, epoch #20, avg. train loss: 2080665.50000\n",
      "Step #200, epoch #40, avg. train loss: 1538628.12500\n",
      "Step #100, epoch #33, avg. train loss: 2474123.75000\n",
      "Step #200, epoch #66, avg. train loss: 1572913.00000\n",
      "Step #100, epoch #33, avg. train loss: 2351851.00000\n",
      "Step #200, epoch #66, avg. train loss: 1542104.37500\n",
      "Step #100, epoch #33, avg. train loss: 2298533.00000\n",
      "Step #200, epoch #66, avg. train loss: 1505431.87500\n",
      "Step #100, epoch #50, avg. train loss: 2230701.50000\n",
      "Step #200, epoch #100, avg. train loss: 1756433.87500\n",
      "Step #100, epoch #50, avg. train loss: 2181811.25000\n",
      "Step #200, epoch #100, avg. train loss: 1546504.00000\n",
      "Step #100, epoch #50, avg. train loss: 2105846.75000\n",
      "Step #200, epoch #100, avg. train loss: 1588990.87500\n",
      "Step #100, epoch #100, avg. train loss: 2344681.00000\n",
      "Step #200, epoch #200, avg. train loss: 1515618.75000\n",
      "Step #100, epoch #100, avg. train loss: 2238671.00000\n",
      "Step #200, epoch #200, avg. train loss: 1507955.62500\n",
      "Step #100, epoch #100, avg. train loss: 2230876.75000\n",
      "Step #200, epoch #200, avg. train loss: 1474126.12500\n",
      "Step #100, epoch #10, avg. train loss: 2764290.00000\n",
      "Step #200, epoch #20, avg. train loss: 1671662.75000\n",
      "Step #100, epoch #10, avg. train loss: 2574723.75000\n",
      "Step #200, epoch #20, avg. train loss: 1560428.75000\n",
      "Step #100, epoch #10, avg. train loss: 2499535.75000\n",
      "Step #200, epoch #20, avg. train loss: 1574986.75000\n",
      "Step #100, epoch #50, avg. train loss: 2996377.25000\n",
      "Step #200, epoch #100, avg. train loss: 1617217.50000\n",
      "Step #100, epoch #50, avg. train loss: 2957856.00000\n",
      "Step #200, epoch #100, avg. train loss: 1531830.87500\n",
      "Step #100, epoch #50, avg. train loss: 2844624.75000\n",
      "Step #200, epoch #100, avg. train loss: 1565186.12500\n",
      "Step #100, epoch #50, avg. train loss: 2454513.00000\n",
      "Step #200, epoch #100, avg. train loss: 1669183.87500\n",
      "Step #100, epoch #50, avg. train loss: 2588548.75000\n",
      "Step #200, epoch #100, avg. train loss: 1627188.37500\n",
      "Step #100, epoch #50, avg. train loss: 2348076.25000\n",
      "Step #200, epoch #100, avg. train loss: 1565525.00000\n",
      "Step #100, epoch #100, avg. train loss: 2175941.75000\n",
      "Step #200, epoch #200, avg. train loss: 1444575.37500\n",
      "Step #100, epoch #100, avg. train loss: 2136618.50000\n",
      "Step #200, epoch #200, avg. train loss: 1508219.50000\n",
      "Step #100, epoch #100, avg. train loss: 2092124.50000\n",
      "Step #200, epoch #200, avg. train loss: 1468785.12500\n",
      "Step #100, epoch #100, avg. train loss: 2632449.25000\n",
      "Step #200, epoch #200, avg. train loss: 1597535.62500\n",
      "Step #100, epoch #100, avg. train loss: 2681010.50000\n",
      "Step #200, epoch #200, avg. train loss: 1552753.00000\n",
      "Step #100, epoch #100, avg. train loss: 2574713.00000\n",
      "Step #200, epoch #200, avg. train loss: 1475973.87500\n",
      "Step #100, epoch #50, avg. train loss: 2546248.25000\n",
      "Step #200, epoch #100, avg. train loss: 1743133.25000\n",
      "Step #100, epoch #50, avg. train loss: 2800269.00000\n",
      "Step #200, epoch #100, avg. train loss: 1563922.25000\n",
      "Step #100, epoch #50, avg. train loss: 3815260.50000\n",
      "Step #200, epoch #100, avg. train loss: 1453784.75000\n",
      "Step #100, epoch #100, avg. train loss: 2795039.25000\n",
      "Step #200, epoch #200, avg. train loss: 1680578.25000\n",
      "Step #100, epoch #100, avg. train loss: 2694012.75000\n",
      "Step #200, epoch #200, avg. train loss: 1473904.62500\n",
      "Step #100, epoch #100, avg. train loss: 2677371.50000\n",
      "Step #200, epoch #200, avg. train loss: 1481945.87500\n",
      "Step #100, epoch #50, avg. train loss: 2080513.87500\n",
      "Step #200, epoch #100, avg. train loss: 1675448.00000\n",
      "Step #100, epoch #50, avg. train loss: 2037192.62500\n",
      "Step #200, epoch #100, avg. train loss: 1575252.62500\n",
      "Step #100, epoch #50, avg. train loss: 1995051.50000\n",
      "Step #200, epoch #100, avg. train loss: 1601652.37500\n",
      "Step #100, epoch #100, avg. train loss: 2085380.12500\n",
      "Step #200, epoch #200, avg. train loss: 1501965.00000\n",
      "Step #100, epoch #100, avg. train loss: 2024497.00000\n",
      "Step #200, epoch #200, avg. train loss: 1542757.62500\n",
      "Step #100, epoch #100, avg. train loss: 1989739.37500\n",
      "Step #200, epoch #200, avg. train loss: 1550605.87500\n",
      "Step #100, epoch #1, avg. train loss: 2585498.50000\n",
      "Step #200, epoch #2, avg. train loss: 2006787.62500\n",
      "Step #100, epoch #1, avg. train loss: 2904155.50000\n",
      "Step #200, epoch #2, avg. train loss: 1832481.12500\n",
      "Step #100, epoch #1, avg. train loss: 2953499.00000\n",
      "Step #200, epoch #2, avg. train loss: 1863440.00000\n",
      "Step #100, epoch #33, avg. train loss: 2300628.00000\n",
      "Step #200, epoch #66, avg. train loss: 1507561.75000\n",
      "Step #100, epoch #33, avg. train loss: 2213624.75000\n",
      "Step #200, epoch #66, avg. train loss: 1473617.25000\n",
      "Step #100, epoch #33, avg. train loss: 2147571.75000\n",
      "Step #200, epoch #66, avg. train loss: 1464533.25000\n",
      "Step #100, epoch #100, avg. train loss: 2704818.00000\n",
      "Step #200, epoch #200, avg. train loss: 1477190.37500\n",
      "Step #100, epoch #100, avg. train loss: 2573355.75000\n",
      "Step #200, epoch #200, avg. train loss: 1418466.50000\n",
      "Step #100, epoch #100, avg. train loss: 2568574.00000\n",
      "Step #200, epoch #200, avg. train loss: 1505060.50000\n",
      "Step #100, epoch #50, avg. train loss: 10916031.00000\n",
      "Step #200, epoch #100, avg. train loss: 9086934.00000\n",
      "Step #100, epoch #50, avg. train loss: 10914013.00000\n",
      "Step #200, epoch #100, avg. train loss: 9127255.00000\n",
      "Step #100, epoch #50, avg. train loss: 10935306.00000\n",
      "Step #200, epoch #100, avg. train loss: 9048877.00000\n",
      "Step #100, epoch #50, avg. train loss: 2123071.50000\n",
      "Step #200, epoch #100, avg. train loss: 1659323.87500\n",
      "Step #100, epoch #50, avg. train loss: 2067985.50000\n",
      "Step #200, epoch #100, avg. train loss: 1585489.87500\n",
      "Step #100, epoch #50, avg. train loss: 2027809.87500\n",
      "Step #200, epoch #100, avg. train loss: 1541723.87500\n",
      "Step #100, epoch #100, avg. train loss: 2900024.25000\n",
      "Step #200, epoch #200, avg. train loss: 1628885.75000\n",
      "Step #100, epoch #100, avg. train loss: 2698559.25000\n",
      "Step #200, epoch #200, avg. train loss: 1539274.87500\n",
      "Step #100, epoch #100, avg. train loss: 2692422.00000\n",
      "Step #200, epoch #200, avg. train loss: 1574059.50000\n",
      "Step #100, epoch #33, avg. train loss: 4218635.50000\n",
      "Step #200, epoch #66, avg. train loss: 1859548.62500\n",
      "Step #100, epoch #33, avg. train loss: 4254786.00000\n",
      "Step #200, epoch #66, avg. train loss: 1773931.25000\n",
      "Step #100, epoch #33, avg. train loss: 4149917.75000\n",
      "Step #200, epoch #66, avg. train loss: 1727167.87500\n",
      "Step #100, epoch #25, avg. train loss: 2834337.00000\n",
      "Step #200, epoch #50, avg. train loss: 1540217.25000\n",
      "Step #100, epoch #25, avg. train loss: 2787574.75000\n",
      "Step #200, epoch #50, avg. train loss: 1556632.12500\n",
      "Step #100, epoch #25, avg. train loss: 2734496.75000\n",
      "Step #200, epoch #50, avg. train loss: 1495747.50000\n",
      "Step #100, epoch #50, avg. train loss: 2267101.00000\n",
      "Step #200, epoch #100, avg. train loss: 1622946.25000\n",
      "Step #100, epoch #50, avg. train loss: 2174500.75000\n",
      "Step #200, epoch #100, avg. train loss: 1537961.00000\n",
      "Step #100, epoch #50, avg. train loss: 2147596.75000\n",
      "Step #200, epoch #100, avg. train loss: 1590442.50000\n",
      "Step #100, epoch #14, avg. train loss: 2611250.50000\n",
      "Step #200, epoch #28, avg. train loss: 1548852.00000\n",
      "Step #100, epoch #14, avg. train loss: 2536675.75000\n",
      "Step #200, epoch #28, avg. train loss: 1561468.00000\n",
      "Step #100, epoch #14, avg. train loss: 2337115.00000\n",
      "Step #200, epoch #28, avg. train loss: 1582232.12500\n",
      "Step #100, epoch #100, avg. train loss: 2259198.00000\n",
      "Step #200, epoch #200, avg. train loss: 1595718.75000\n",
      "Step #100, epoch #100, avg. train loss: 2171707.75000\n",
      "Step #200, epoch #200, avg. train loss: 1536030.25000\n",
      "Step #100, epoch #100, avg. train loss: 2136864.00000\n",
      "Step #200, epoch #200, avg. train loss: 1528912.75000\n",
      "Step #100, epoch #33, avg. train loss: 4052464.25000\n",
      "Step #200, epoch #66, avg. train loss: 1604271.50000\n",
      "Step #100, epoch #33, avg. train loss: 3714551.25000\n",
      "Step #200, epoch #66, avg. train loss: 1541273.75000\n",
      "Step #100, epoch #33, avg. train loss: 3596512.00000\n",
      "Step #200, epoch #66, avg. train loss: 1543180.62500\n",
      "Step #100, epoch #50, avg. train loss: 2885744.25000\n",
      "Step #200, epoch #100, avg. train loss: 1571879.62500\n",
      "Step #100, epoch #50, avg. train loss: 2877677.00000\n",
      "Step #200, epoch #100, avg. train loss: 1407518.87500\n",
      "Step #100, epoch #50, avg. train loss: 2876365.50000\n",
      "Step #200, epoch #100, avg. train loss: 1516278.25000\n",
      "Step #100, epoch #100, avg. train loss: 2908126.75000\n",
      "Step #200, epoch #200, avg. train loss: 1482389.50000\n",
      "Step #100, epoch #100, avg. train loss: 2918483.50000\n",
      "Step #200, epoch #200, avg. train loss: 1494272.00000\n",
      "Step #100, epoch #100, avg. train loss: 3100538.25000\n",
      "Step #200, epoch #200, avg. train loss: 1585488.62500\n",
      "Step #100, epoch #33, avg. train loss: 2962098.50000\n",
      "Step #200, epoch #66, avg. train loss: 1544488.50000\n",
      "Step #100, epoch #33, avg. train loss: 2709583.75000\n",
      "Step #200, epoch #66, avg. train loss: 1481682.12500\n",
      "Step #100, epoch #33, avg. train loss: 2665675.00000\n",
      "Step #200, epoch #66, avg. train loss: 1417119.25000\n",
      "Step #100, epoch #50, avg. train loss: 2126818.75000\n",
      "Step #200, epoch #100, avg. train loss: 1533988.62500\n",
      "Step #100, epoch #50, avg. train loss: 2090689.87500\n",
      "Step #200, epoch #100, avg. train loss: 1488959.00000\n",
      "Step #100, epoch #50, avg. train loss: 2024204.62500\n",
      "Step #200, epoch #100, avg. train loss: 1506953.12500\n",
      "Step #100, epoch #50, avg. train loss: 2233723.50000\n",
      "Step #200, epoch #100, avg. train loss: 1561252.50000\n",
      "Step #100, epoch #50, avg. train loss: 2150198.00000\n",
      "Step #200, epoch #100, avg. train loss: 1464719.00000\n",
      "Step #100, epoch #50, avg. train loss: 2104442.25000\n",
      "Step #200, epoch #100, avg. train loss: 1559732.75000\n",
      "Step #100, epoch #100, avg. train loss: 2453229.00000\n",
      "Step #200, epoch #200, avg. train loss: 1632962.12500\n",
      "Step #100, epoch #100, avg. train loss: 2483288.00000\n",
      "Step #200, epoch #200, avg. train loss: 1625717.25000\n",
      "Step #100, epoch #100, avg. train loss: 2537892.25000\n",
      "Step #200, epoch #200, avg. train loss: 1632095.00000\n",
      "Step #100, epoch #33, avg. train loss: 2215351.75000\n",
      "Step #200, epoch #66, avg. train loss: 1720625.75000\n",
      "Step #100, epoch #33, avg. train loss: 2133419.50000\n",
      "Step #200, epoch #66, avg. train loss: 1674441.87500\n",
      "Step #100, epoch #33, avg. train loss: 2037467.25000\n",
      "Step #200, epoch #66, avg. train loss: 1608971.62500\n",
      "Step #100, epoch #100, avg. train loss: 5163424.00000\n",
      "Step #200, epoch #200, avg. train loss: 1821789.12500\n",
      "Step #100, epoch #100, avg. train loss: 5144235.00000\n",
      "Step #200, epoch #200, avg. train loss: 1740875.87500\n",
      "Step #100, epoch #100, avg. train loss: 5079452.00000\n",
      "Step #200, epoch #200, avg. train loss: 1696992.00000\n",
      "Step #100, epoch #50, avg. train loss: 2224034.00000\n",
      "Step #200, epoch #100, avg. train loss: 1782526.25000\n",
      "Step #100, epoch #50, avg. train loss: 2162821.50000\n",
      "Step #200, epoch #100, avg. train loss: 1623355.25000\n",
      "Step #100, epoch #50, avg. train loss: 2087183.62500\n",
      "Step #200, epoch #100, avg. train loss: 1631488.00000\n",
      "Step #100, epoch #100, avg. train loss: 2305843.25000\n",
      "Step #200, epoch #200, avg. train loss: 1560629.62500\n",
      "Step #100, epoch #100, avg. train loss: 2198008.25000\n",
      "Step #200, epoch #200, avg. train loss: 1478799.87500\n",
      "Step #100, epoch #100, avg. train loss: 2182126.50000\n",
      "Step #200, epoch #200, avg. train loss: 1486468.75000\n",
      "Step #100, epoch #100, avg. train loss: 2893008.25000\n",
      "Step #200, epoch #200, avg. train loss: 1647590.37500\n",
      "Step #100, epoch #100, avg. train loss: 2788709.50000\n",
      "Step #200, epoch #200, avg. train loss: 1467553.50000\n",
      "Step #100, epoch #100, avg. train loss: 2788907.75000\n",
      "Step #200, epoch #200, avg. train loss: 1452331.87500\n",
      "Step #100, epoch #50, avg. train loss: 2946255.00000\n",
      "Step #200, epoch #100, avg. train loss: 1845542.12500\n",
      "Step #100, epoch #50, avg. train loss: 2883547.50000\n",
      "Step #200, epoch #100, avg. train loss: 1751382.12500\n",
      "Step #100, epoch #50, avg. train loss: 2838118.75000\n",
      "Step #200, epoch #100, avg. train loss: 1704784.12500\n",
      "Step #100, epoch #100, avg. train loss: 2881640.00000\n",
      "Step #200, epoch #200, avg. train loss: 1606381.12500\n",
      "Step #100, epoch #100, avg. train loss: 2692688.00000\n",
      "Step #200, epoch #200, avg. train loss: 1512529.00000\n",
      "Step #100, epoch #100, avg. train loss: 2690840.00000\n",
      "Step #200, epoch #200, avg. train loss: 1537300.50000\n",
      "Step #100, epoch #100, avg. train loss: 2805395.50000\n",
      "Step #200, epoch #200, avg. train loss: 1532760.75000\n",
      "Step #100, epoch #100, avg. train loss: 2714906.50000\n",
      "Step #200, epoch #200, avg. train loss: 1502757.75000\n",
      "Step #100, epoch #100, avg. train loss: 2741661.00000\n",
      "Step #200, epoch #200, avg. train loss: 1580057.00000\n",
      "Step #100, epoch #20, avg. train loss: 2151662.25000\n",
      "Step #200, epoch #40, avg. train loss: 1729133.87500\n",
      "Step #100, epoch #20, avg. train loss: 2063552.50000\n",
      "Step #200, epoch #40, avg. train loss: 1609828.12500\n",
      "Step #100, epoch #20, avg. train loss: 2028407.62500\n",
      "Step #200, epoch #40, avg. train loss: 1570737.00000\n",
      "Step #100, epoch #100, avg. train loss: 3212633.00000\n",
      "Step #200, epoch #200, avg. train loss: 1480442.37500\n",
      "Step #100, epoch #100, avg. train loss: 2893426.00000\n",
      "Step #200, epoch #200, avg. train loss: 1445063.62500\n",
      "Step #100, epoch #100, avg. train loss: 2898155.25000\n",
      "Step #200, epoch #200, avg. train loss: 1479167.37500\n",
      "Step #100, epoch #100, avg. train loss: 2817559.00000\n",
      "Step #200, epoch #200, avg. train loss: 1515677.00000\n",
      "Step #100, epoch #100, avg. train loss: 2686115.00000\n",
      "Step #200, epoch #200, avg. train loss: 1451637.25000\n",
      "Step #100, epoch #100, avg. train loss: 2698299.75000\n",
      "Step #200, epoch #200, avg. train loss: 1459705.75000\n",
      "Step #100, epoch #100, avg. train loss: 2394000.75000\n",
      "Step #200, epoch #200, avg. train loss: 1566037.62500\n",
      "Step #100, epoch #100, avg. train loss: 2329794.00000\n",
      "Step #200, epoch #200, avg. train loss: 1537133.25000\n",
      "Step #100, epoch #100, avg. train loss: 2284417.75000\n",
      "Step #200, epoch #200, avg. train loss: 1496515.50000\n",
      "Step #100, epoch #50, avg. train loss: 3143122.25000\n",
      "Step #200, epoch #100, avg. train loss: 1540814.75000\n",
      "Step #100, epoch #50, avg. train loss: 3008607.00000\n",
      "Step #200, epoch #100, avg. train loss: 1518312.00000\n",
      "Step #100, epoch #50, avg. train loss: 2978416.00000\n",
      "Step #200, epoch #100, avg. train loss: 1556011.87500\n",
      "Step #100, epoch #100, avg. train loss: 2172141.00000\n",
      "Step #200, epoch #200, avg. train loss: 1533807.50000\n",
      "Step #100, epoch #100, avg. train loss: 2078889.62500\n",
      "Step #200, epoch #200, avg. train loss: 1482076.50000\n",
      "Step #100, epoch #100, avg. train loss: 2048951.62500\n",
      "Step #200, epoch #200, avg. train loss: 1470809.12500\n",
      "Step #100, epoch #20, avg. train loss: 3021870.50000\n",
      "Step #200, epoch #40, avg. train loss: 1606678.75000\n",
      "Step #100, epoch #20, avg. train loss: 3010569.50000\n",
      "Step #200, epoch #40, avg. train loss: 1525457.25000\n",
      "Step #100, epoch #20, avg. train loss: 2864611.00000\n",
      "Step #200, epoch #40, avg. train loss: 1434927.50000\n",
      "Step #100, epoch #100, avg. train loss: 9401595.00000\n",
      "Step #200, epoch #200, avg. train loss: 4386062.50000\n",
      "Step #100, epoch #100, avg. train loss: 9398149.00000\n",
      "Step #200, epoch #200, avg. train loss: 4417325.50000\n",
      "Step #100, epoch #100, avg. train loss: 9347028.00000\n",
      "Step #200, epoch #200, avg. train loss: 4320343.50000\n",
      "Step #100, epoch #50, avg. train loss: 2517860.25000\n",
      "Step #200, epoch #100, avg. train loss: 1651024.62500\n",
      "Step #100, epoch #50, avg. train loss: 2323789.50000\n",
      "Step #200, epoch #100, avg. train loss: 1549489.25000\n",
      "Step #100, epoch #50, avg. train loss: 2292833.75000\n",
      "Step #200, epoch #100, avg. train loss: 1561322.75000\n",
      "Step #100, epoch #100, avg. train loss: 2247979.50000\n",
      "Step #200, epoch #200, avg. train loss: 1593358.37500\n",
      "Step #100, epoch #100, avg. train loss: 2157945.75000\n",
      "Step #200, epoch #200, avg. train loss: 1545397.00000\n",
      "Step #100, epoch #100, avg. train loss: 2142628.75000\n",
      "Step #200, epoch #200, avg. train loss: 1490944.62500\n",
      "Step #100, epoch #50, avg. train loss: 3617218.50000\n",
      "Step #200, epoch #100, avg. train loss: 1710671.37500\n",
      "Step #100, epoch #50, avg. train loss: 3069547.75000\n",
      "Step #200, epoch #100, avg. train loss: 1587454.50000\n",
      "Step #100, epoch #50, avg. train loss: 3376486.00000\n",
      "Step #200, epoch #100, avg. train loss: 1608789.00000\n",
      "Step #100, epoch #100, avg. train loss: 11732906.00000\n",
      "Step #200, epoch #200, avg. train loss: 11317612.00000\n",
      "Step #100, epoch #100, avg. train loss: 11688809.00000\n",
      "Step #200, epoch #200, avg. train loss: 11281688.00000\n",
      "Step #100, epoch #100, avg. train loss: 11672684.00000\n",
      "Step #200, epoch #200, avg. train loss: 11258712.00000\n",
      "Step #100, epoch #33, avg. train loss: 2542075.75000\n",
      "Step #200, epoch #66, avg. train loss: 1556556.50000\n",
      "Step #100, epoch #33, avg. train loss: 2475535.00000\n",
      "Step #200, epoch #66, avg. train loss: 1572270.37500\n",
      "Step #100, epoch #33, avg. train loss: 2330864.75000\n",
      "Step #200, epoch #66, avg. train loss: 1588153.87500\n",
      "Step #100, epoch #50, avg. train loss: 2227091.50000\n",
      "Step #200, epoch #100, avg. train loss: 1498362.87500\n",
      "Step #100, epoch #50, avg. train loss: 2152801.50000\n",
      "Step #200, epoch #100, avg. train loss: 1480490.75000\n",
      "Step #100, epoch #50, avg. train loss: 2099302.75000\n",
      "Step #200, epoch #100, avg. train loss: 1543001.12500\n",
      "Step #100, epoch #100, avg. train loss: 10239247.00000\n",
      "Step #200, epoch #200, avg. train loss: 8582026.00000\n",
      "Step #100, epoch #100, avg. train loss: 10237334.00000\n",
      "Step #200, epoch #200, avg. train loss: 8683849.00000\n",
      "Step #100, epoch #100, avg. train loss: 10187100.00000\n",
      "Step #200, epoch #200, avg. train loss: 8533145.00000\n",
      "Step #100, epoch #100, avg. train loss: 2207859.75000\n",
      "Step #200, epoch #200, avg. train loss: 1544922.37500\n",
      "Step #100, epoch #100, avg. train loss: 2105347.25000\n",
      "Step #200, epoch #200, avg. train loss: 1492846.87500\n",
      "Step #100, epoch #100, avg. train loss: 2065599.25000\n",
      "Step #200, epoch #200, avg. train loss: 1459110.25000\n",
      "Step #100, epoch #100, avg. train loss: 2067238.50000\n",
      "Step #200, epoch #200, avg. train loss: 1493060.50000\n",
      "best CV score from grid search: 0.322974\n",
      "corresponding parameters: {'learning_rate': 0.3560042443718171, 'hidden_units': [14, 14], 'batch_size': 712}\n",
      "Score: 0.175321\n"
     ]
    }
   ],
   "source": [
    "# DNN-Regressor tuned with RandomizesSearch\n",
    "\n",
    "# Parameters\n",
    "param_dist = {  'hidden_units': [[13,13], [14,14], [15,15]], \n",
    "                'learning_rate': sp_uniform(0.0,1.0), \n",
    "                'batch_size': sp_randint(1,729)\n",
    "             }\n",
    "\n",
    "n_iter_search = 10\n",
    "\n",
    "# MSE optimized\n",
    "#regressor_tuned_RS = RandomizedSearchCV(skflow.TensorFlowDNNRegressor (hidden_units = [10, 10]), param_distributions = param_dist, scoring = 'mean_squared_error', n_iter=n_iter_search)\n",
    "\n",
    "# R^2 optimized\n",
    "regressor_tuned_RS = RandomizedSearchCV(skflow.TensorFlowDNNRegressor (hidden_units = [10, 10]), param_distributions = param_dist, scoring = 'r2', n_iter=n_iter_search)\n",
    "\n",
    "# Fit\n",
    "regressor_tuned_RS.fit(X_train, y_train)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(regressor_tuned_RS.best_score_))\n",
    "print('corresponding parameters: {}'.format(regressor_tuned_RS.best_params_))\n",
    "\n",
    "# source: https://github.com/tensorflow/skflow/pull/126/files\n",
    "\n",
    "# Predict and score\n",
    "predict = regressor_tuned_RS.predict(X_test)\n",
    "\n",
    "#score_regressor_tuned_RS = mean_squared_error(y_test, predict)\n",
    "score_regressor_tuned_RS = r2_score(y_test, predict)\n",
    "\n",
    "print('Score: {0:f}'.format(score_regressor_tuned_RS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Example results of RS tuned DNN-Regressor: \n",
    " \n",
    "corresponding parameters: {'learning_rate': 0.7992653962585401, 'hidden_units': [13, 13], 'batch_size': 561}\n",
    "MSE: 3257365.518337\n",
    "\n",
    "corresponding parameters: {'learning_rate': 0.9620614315321504, 'hidden_units': [14, 14], 'batch_size': 413}\n",
    "MSE: 1991265.709509  \n",
    "\n",
    "corresponding parameters: {'learning_rate': 0.9874801602812813, 'hidden_units': [14, 14], 'batch_size': 445}\n",
    "MSE: 2543202.004386\n",
    "\n",
    "corresponding parameters: {'learning_rate': 0.9167557134938925, 'hidden_units': [14, 14], 'batch_size': 424}\n",
    "score: 0.404403   \n",
    "\n",
    "corresponding parameters: {'learning_rate': 0.7574216583764648, 'hidden_units': [14, 14], 'batch_size': 573}\n",
    "Score: 0.306644\n",
    "\n",
    "corresponding parameters: {'learning_rate': 0.7579695776853932, 'hidden_units': [14, 14], 'batch_size': 537}\n",
    "Score: 0.185412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100, epoch #50, avg. train loss: 2744291.00000\n",
      "Step #100, epoch #50, avg. train loss: 2624199.75000\n",
      "Step #100, epoch #50, avg. train loss: 2759042.50000\n",
      "Step #100, epoch #50, avg. train loss: 2131744.00000\n",
      "Step #100, epoch #50, avg. train loss: 2024819.00000\n",
      "Step #100, epoch #50, avg. train loss: 2008927.50000\n",
      "Step #100, epoch #50, avg. train loss: 2498375.50000\n",
      "Step #100, epoch #50, avg. train loss: 2419363.00000\n",
      "Step #100, epoch #50, avg. train loss: 2380122.00000\n",
      "Step #100, epoch #50, avg. train loss: 2862329.50000\n",
      "Step #100, epoch #50, avg. train loss: 2872755.50000\n",
      "Step #100, epoch #50, avg. train loss: 2798281.50000\n",
      "Step #100, epoch #50, avg. train loss: 2091379.00000\n",
      "Step #100, epoch #50, avg. train loss: 2020791.25000\n",
      "Step #100, epoch #50, avg. train loss: 2008557.62500\n",
      "Step #100, epoch #50, avg. train loss: 2540901.50000\n",
      "Step #100, epoch #50, avg. train loss: 2467552.25000\n",
      "Step #100, epoch #50, avg. train loss: 2431149.50000\n",
      "Step #100, epoch #50, avg. train loss: 3880615.75000\n",
      "Step #100, epoch #50, avg. train loss: 3728483.00000\n",
      "Step #100, epoch #50, avg. train loss: 3893261.50000\n",
      "Step #100, epoch #50, avg. train loss: 2117688.25000\n",
      "Step #100, epoch #50, avg. train loss: 2068102.87500\n",
      "Step #100, epoch #50, avg. train loss: 2049259.50000\n",
      "Step #100, epoch #50, avg. train loss: 2641040.25000\n",
      "Step #100, epoch #50, avg. train loss: 2559345.75000\n",
      "Step #100, epoch #50, avg. train loss: 2526198.50000\n",
      "Step #100, epoch #50, avg. train loss: 3146882.50000\n",
      "Step #100, epoch #50, avg. train loss: 3011603.50000\n",
      "Step #100, epoch #50, avg. train loss: 2943827.00000\n",
      "Step #100, epoch #50, avg. train loss: 2135328.75000\n",
      "Step #100, epoch #50, avg. train loss: 2030364.50000\n",
      "Step #100, epoch #50, avg. train loss: 2008645.00000\n",
      "Step #100, epoch #50, avg. train loss: 2322839.75000\n",
      "Step #100, epoch #50, avg. train loss: 2233294.50000\n",
      "Step #100, epoch #50, avg. train loss: 2202464.50000\n",
      "Step #100, epoch #50, avg. train loss: 3347155.50000\n",
      "Step #100, epoch #50, avg. train loss: 3355742.00000\n",
      "Step #100, epoch #50, avg. train loss: 2805346.50000\n",
      "Step #100, epoch #50, avg. train loss: 2130839.75000\n",
      "Step #100, epoch #50, avg. train loss: 2047164.62500\n",
      "Step #100, epoch #50, avg. train loss: 2013066.25000\n",
      "Step #100, epoch #50, avg. train loss: 2409101.25000\n",
      "Step #100, epoch #50, avg. train loss: 2329243.50000\n",
      "Step #100, epoch #50, avg. train loss: 2291800.50000\n",
      "Step #100, epoch #50, avg. train loss: 2852195.00000\n",
      "Step #100, epoch #50, avg. train loss: 2729388.50000\n",
      "Step #100, epoch #50, avg. train loss: 2770747.00000\n",
      "Step #100, epoch #50, avg. train loss: 2085664.50000\n",
      "Step #100, epoch #50, avg. train loss: 2034939.50000\n",
      "Step #100, epoch #50, avg. train loss: 2022052.62500\n",
      "Step #100, epoch #50, avg. train loss: 2458094.00000\n",
      "Step #100, epoch #50, avg. train loss: 2417233.50000\n",
      "Step #100, epoch #50, avg. train loss: 2364679.75000\n",
      "Step #100, epoch #50, avg. train loss: 2841608.00000\n",
      "Step #100, epoch #50, avg. train loss: 2800559.25000\n",
      "Step #100, epoch #50, avg. train loss: 2922730.50000\n",
      "Step #100, epoch #50, avg. train loss: 2084893.75000\n",
      "Step #100, epoch #50, avg. train loss: 2045174.25000\n",
      "Step #100, epoch #50, avg. train loss: 2008192.37500\n",
      "Step #100, epoch #50, avg. train loss: 2499764.25000\n",
      "Step #100, epoch #50, avg. train loss: 2460156.00000\n",
      "Step #100, epoch #50, avg. train loss: 2412954.75000\n",
      "Step #100, epoch #50, avg. train loss: 3876556.25000\n",
      "Step #100, epoch #50, avg. train loss: 3978662.75000\n",
      "Step #100, epoch #50, avg. train loss: 3988856.75000\n",
      "Step #100, epoch #50, avg. train loss: 2115852.25000\n",
      "Step #100, epoch #50, avg. train loss: 2045468.00000\n",
      "Step #100, epoch #50, avg. train loss: 2020072.00000\n",
      "Step #100, epoch #50, avg. train loss: 2600571.75000\n",
      "Step #100, epoch #50, avg. train loss: 2549465.50000\n",
      "Step #100, epoch #50, avg. train loss: 2503280.75000\n",
      "Step #100, epoch #50, avg. train loss: 3081645.75000\n",
      "Step #100, epoch #50, avg. train loss: 3046536.25000\n",
      "Step #100, epoch #50, avg. train loss: 3086638.50000\n",
      "Step #100, epoch #50, avg. train loss: 2114209.50000\n",
      "Step #100, epoch #50, avg. train loss: 2023022.75000\n",
      "Step #100, epoch #50, avg. train loss: 1975968.00000\n",
      "Step #100, epoch #50, avg. train loss: 2287904.25000\n",
      "Step #100, epoch #50, avg. train loss: 2240722.50000\n",
      "Step #100, epoch #50, avg. train loss: 2187657.00000\n",
      "Step #100, epoch #50, avg. train loss: 2752756.25000\n",
      "Step #100, epoch #50, avg. train loss: 3291585.00000\n",
      "Step #100, epoch #50, avg. train loss: 2794419.50000\n",
      "Step #100, epoch #50, avg. train loss: 2121061.50000\n",
      "Step #100, epoch #50, avg. train loss: 2026896.37500\n",
      "Step #100, epoch #50, avg. train loss: 1987488.37500\n",
      "Step #100, epoch #50, avg. train loss: 2379200.75000\n",
      "Step #100, epoch #50, avg. train loss: 2315211.75000\n",
      "Step #100, epoch #50, avg. train loss: 2272842.00000\n",
      "Step #100, epoch #50, avg. train loss: 3252832.25000\n",
      "Step #100, epoch #50, avg. train loss: 2833810.00000\n",
      "Step #100, epoch #50, avg. train loss: 2841145.25000\n",
      "Step #100, epoch #50, avg. train loss: 2163717.75000\n",
      "Step #100, epoch #50, avg. train loss: 2101911.00000\n",
      "Step #100, epoch #50, avg. train loss: 2030334.37500\n",
      "Step #100, epoch #50, avg. train loss: 2491004.75000\n",
      "Step #100, epoch #50, avg. train loss: 2436267.25000\n",
      "Step #100, epoch #50, avg. train loss: 2365737.75000\n",
      "Step #100, epoch #50, avg. train loss: 2980907.50000\n",
      "Step #100, epoch #50, avg. train loss: 2844577.00000\n",
      "Step #100, epoch #50, avg. train loss: 2981268.25000\n",
      "Step #100, epoch #50, avg. train loss: 2164187.75000\n",
      "Step #100, epoch #50, avg. train loss: 2080982.37500\n",
      "Step #100, epoch #50, avg. train loss: 1993908.12500\n",
      "Step #100, epoch #50, avg. train loss: 2533861.50000\n",
      "Step #100, epoch #50, avg. train loss: 2505059.25000\n",
      "Step #100, epoch #50, avg. train loss: 2428140.75000\n",
      "Step #100, epoch #50, avg. train loss: 4365298.50000\n",
      "Step #100, epoch #50, avg. train loss: 3875347.25000\n",
      "Step #100, epoch #50, avg. train loss: 3999028.25000\n",
      "Step #100, epoch #50, avg. train loss: 2163754.00000\n",
      "Step #100, epoch #50, avg. train loss: 2091037.25000\n",
      "Step #100, epoch #50, avg. train loss: 2020818.87500\n",
      "Step #100, epoch #50, avg. train loss: 2630071.25000\n",
      "Step #100, epoch #50, avg. train loss: 2576651.00000\n",
      "Step #100, epoch #50, avg. train loss: 2515379.75000\n",
      "Step #100, epoch #50, avg. train loss: 3142544.00000\n",
      "Step #100, epoch #50, avg. train loss: 3265991.75000\n",
      "Step #100, epoch #50, avg. train loss: 3047270.50000\n",
      "Step #100, epoch #50, avg. train loss: 2158971.75000\n",
      "Step #100, epoch #50, avg. train loss: 2096581.62500\n",
      "Step #100, epoch #50, avg. train loss: 2005153.62500\n",
      "Step #100, epoch #50, avg. train loss: 2351353.75000\n",
      "Step #100, epoch #50, avg. train loss: 2276448.75000\n",
      "Step #100, epoch #50, avg. train loss: 2191195.00000\n",
      "Step #100, epoch #50, avg. train loss: 2687375.75000\n",
      "Step #100, epoch #50, avg. train loss: 3126819.50000\n",
      "Step #100, epoch #50, avg. train loss: 3179020.75000\n",
      "Step #100, epoch #50, avg. train loss: 2182403.00000\n",
      "Step #100, epoch #50, avg. train loss: 2104539.50000\n",
      "Step #100, epoch #50, avg. train loss: 2028608.75000\n",
      "Step #100, epoch #50, avg. train loss: 2422564.50000\n",
      "Step #100, epoch #50, avg. train loss: 2367885.75000\n",
      "Step #100, epoch #50, avg. train loss: 2273579.00000\n",
      "Step #100, epoch #100, avg. train loss: 3008776.75000\n",
      "Step #100, epoch #100, avg. train loss: 2589954.75000\n",
      "Step #100, epoch #100, avg. train loss: 2609061.00000\n",
      "Step #100, epoch #100, avg. train loss: 2090234.75000\n",
      "Step #100, epoch #100, avg. train loss: 2008210.25000\n",
      "Step #100, epoch #100, avg. train loss: 1989822.75000\n",
      "Step #100, epoch #100, avg. train loss: 2471024.50000\n",
      "Step #100, epoch #100, avg. train loss: 2398758.50000\n",
      "Step #100, epoch #100, avg. train loss: 2353419.25000\n",
      "Step #100, epoch #100, avg. train loss: 2941470.50000\n",
      "Step #100, epoch #100, avg. train loss: 2856458.50000\n",
      "Step #100, epoch #100, avg. train loss: 2840087.25000\n",
      "Step #100, epoch #100, avg. train loss: 2109907.00000\n",
      "Step #100, epoch #100, avg. train loss: 2004519.00000\n",
      "Step #100, epoch #100, avg. train loss: 1991881.87500\n",
      "Step #100, epoch #100, avg. train loss: 2515700.50000\n",
      "Step #100, epoch #100, avg. train loss: 2450173.00000\n",
      "Step #100, epoch #100, avg. train loss: 2402571.75000\n",
      "Step #100, epoch #100, avg. train loss: 3908752.00000\n",
      "Step #100, epoch #100, avg. train loss: 3755179.25000\n",
      "Step #100, epoch #100, avg. train loss: 3813297.50000\n",
      "Step #100, epoch #100, avg. train loss: 2143457.75000\n",
      "Step #100, epoch #100, avg. train loss: 2053023.62500\n",
      "Step #100, epoch #100, avg. train loss: 2022300.50000\n",
      "Step #100, epoch #100, avg. train loss: 2626162.00000\n",
      "Step #100, epoch #100, avg. train loss: 2532948.50000\n",
      "Step #100, epoch #100, avg. train loss: 2490539.00000\n",
      "Step #100, epoch #100, avg. train loss: 3034386.25000\n",
      "Step #100, epoch #100, avg. train loss: 2802191.75000\n",
      "Step #100, epoch #100, avg. train loss: 2868866.50000\n",
      "Step #100, epoch #100, avg. train loss: 2130021.50000\n",
      "Step #100, epoch #100, avg. train loss: 2040224.12500\n",
      "Step #100, epoch #100, avg. train loss: 2004702.12500\n",
      "Step #100, epoch #100, avg. train loss: 2301554.00000\n",
      "Step #100, epoch #100, avg. train loss: 2227484.25000\n",
      "Step #100, epoch #100, avg. train loss: 2187032.75000\n",
      "Step #100, epoch #100, avg. train loss: 2708662.00000\n",
      "Step #100, epoch #100, avg. train loss: 3223609.50000\n",
      "Step #100, epoch #100, avg. train loss: 3534683.50000\n",
      "Step #100, epoch #100, avg. train loss: 2101487.25000\n",
      "Step #100, epoch #100, avg. train loss: 2028022.25000\n",
      "Step #100, epoch #100, avg. train loss: 1979442.50000\n",
      "Step #100, epoch #100, avg. train loss: 2388795.75000\n",
      "Step #100, epoch #100, avg. train loss: 2306231.00000\n",
      "Step #100, epoch #100, avg. train loss: 2275050.00000\n",
      "Step #100, epoch #100, avg. train loss: 3008776.75000\n",
      "Step #100, epoch #100, avg. train loss: 2589954.75000\n",
      "Step #100, epoch #100, avg. train loss: 2609061.00000\n",
      "Step #100, epoch #100, avg. train loss: 2090234.75000\n",
      "Step #100, epoch #100, avg. train loss: 2008210.25000\n",
      "Step #100, epoch #100, avg. train loss: 1989822.75000\n",
      "Step #100, epoch #100, avg. train loss: 2471024.50000\n",
      "Step #100, epoch #100, avg. train loss: 2398758.50000\n",
      "Step #100, epoch #100, avg. train loss: 2353419.25000\n",
      "Step #100, epoch #100, avg. train loss: 2941470.50000\n",
      "Step #100, epoch #100, avg. train loss: 2856458.50000\n",
      "Step #100, epoch #100, avg. train loss: 2840087.25000\n",
      "Step #100, epoch #100, avg. train loss: 2109907.00000\n",
      "Step #100, epoch #100, avg. train loss: 2004519.00000\n",
      "Step #100, epoch #100, avg. train loss: 1991881.87500\n",
      "Step #100, epoch #100, avg. train loss: 2515700.50000\n",
      "Step #100, epoch #100, avg. train loss: 2450173.00000\n",
      "Step #100, epoch #100, avg. train loss: 2402571.75000\n",
      "Step #100, epoch #100, avg. train loss: 3908752.00000\n",
      "Step #100, epoch #100, avg. train loss: 3755179.25000\n",
      "Step #100, epoch #100, avg. train loss: 3813297.50000\n",
      "Step #100, epoch #100, avg. train loss: 2143457.75000\n",
      "Step #100, epoch #100, avg. train loss: 2053023.62500\n",
      "Step #100, epoch #100, avg. train loss: 2022300.50000\n",
      "Step #100, epoch #100, avg. train loss: 2626162.00000\n",
      "Step #100, epoch #100, avg. train loss: 2532948.50000\n",
      "Step #100, epoch #100, avg. train loss: 2490539.00000\n",
      "Step #100, epoch #100, avg. train loss: 3034386.25000\n",
      "Step #100, epoch #100, avg. train loss: 2802191.75000\n",
      "Step #100, epoch #100, avg. train loss: 2868866.50000\n",
      "Step #100, epoch #100, avg. train loss: 2130021.50000\n",
      "Step #100, epoch #100, avg. train loss: 2040224.12500\n",
      "Step #100, epoch #100, avg. train loss: 2004702.12500\n",
      "Step #100, epoch #100, avg. train loss: 2301554.00000\n",
      "Step #100, epoch #100, avg. train loss: 2227484.25000\n",
      "Step #100, epoch #100, avg. train loss: 2187032.75000\n",
      "Step #100, epoch #100, avg. train loss: 2708662.00000\n",
      "Step #100, epoch #100, avg. train loss: 3223609.50000\n",
      "Step #100, epoch #100, avg. train loss: 3534683.50000\n",
      "Step #100, epoch #100, avg. train loss: 2101487.25000\n",
      "Step #100, epoch #100, avg. train loss: 2028022.25000\n",
      "Step #100, epoch #100, avg. train loss: 1979442.50000\n",
      "Step #100, epoch #100, avg. train loss: 2388795.75000\n",
      "Step #100, epoch #100, avg. train loss: 2306231.00000\n",
      "Step #100, epoch #100, avg. train loss: 2275050.00000\n",
      "Step #100, epoch #50, avg. train loss: 3872568.00000\n",
      "best CV score from grid search: 0.178553\n",
      "corresponding parameters: {'steps': 100, 'learning_rate': 1.0, 'hidden_units': [13, 13], 'batch_size': 400}\n",
      "Score: 0.046538\n"
     ]
    }
   ],
   "source": [
    "# DNN-Regressor tuned with GS\n",
    "\n",
    "# param_grid\n",
    "param_grid = {'hidden_units': [[11,11], [12,12], [13,13], [14,14], [15,15]], \n",
    "              'steps': [100],\n",
    "              'learning_rate': [1.0, 0.7, 0.3],\n",
    "              'batch_size': [250, 300, 350, 400, 450]\n",
    "             }\n",
    "\n",
    "# GS with MSE\n",
    "#regressor_tuned = GridSearchCV(skflow.TensorFlowDNNRegressor (hidden_units = [10, 10]), param_grid, scoring = 'mean_squared_error')\n",
    "\n",
    "# GS with R^2\n",
    "regressor_tuned_GS = GridSearchCV(skflow.TensorFlowDNNRegressor (hidden_units = [10, 10]), param_grid, scoring = 'r2')\n",
    "\n",
    "# Fit\n",
    "regressor_tuned_GS.fit(X_train, y_train)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(regressor_tuned_GS.best_score_))\n",
    "print('corresponding parameters: {}'.format(regressor_tuned_GS.best_params_))\n",
    "\n",
    "# source: https://github.com/tensorflow/skflow/pull/126/files\n",
    "\n",
    "# Predict and score\n",
    "predict = regressor_tuned_GS.predict(X_test)\n",
    "\n",
    "#score_regressor_tuned_GS = mean_squared_error(y_test, predict)\n",
    "score_regressor_tuned_GS = r2_score(y_test, predict)\n",
    "\n",
    "print('Score: {0:f}'.format(score_regressor_tuned_GS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Exsample results of GS tuned DNN Regressor:\n",
    "\n",
    "corresponding parameters: {'steps': 100, 'learning_rate': 1.0, 'hidden_units': [13, 13], 'batch_size': 400}\n",
    "MSE: 3275144.565188\n",
    "\n",
    "corresponding parameters: {'steps': 100, 'learning_rate': 1.0, 'hidden_units': [13, 13], 'batch_size': 400}\n",
    "MSE: 3275144.565188  \n",
    "\n",
    "corresponding parameters: {'steps': 100, 'learning_rate': 1.0, 'hidden_units': [12, 12], 'batch_size': 400}\n",
    "MSE: 2115469.864387  \n",
    "\n",
    "corresponding parameters: {'steps': 100, 'learning_rate': 1.0, 'hidden_units': [13, 13], 'batch_size': 400}\n",
    "Score: 0.284954\n",
    "\n",
    "corresponding parameters: {'steps': 100, 'learning_rate': 1.0, 'hidden_units': [13, 13], 'batch_size': 400}\n",
    "Score: 0.183576\n",
    "\n",
    "corresponding parameters: {'steps': 100, 'learning_rate': 1.0, 'hidden_units': [13, 13], 'batch_size': 400}\n",
    "Score: 0.019807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR: -0.001130\n",
      "SVR tuned: 0.245852\n",
      "DNN: -0.463030\n",
      "DNN tuned random: 0.175321\n",
      "DNN tuned grid: 0.046538\n"
     ]
    }
   ],
   "source": [
    "print(\"SVR: %f\" % score_svr)\n",
    "print(\"SVR tuned: %f\" % score_svr_tuned)\n",
    "\n",
    "print(\"DNN: %f\" % score_regressor)\n",
    "print(\"DNN tuned random: %f\" % score_regressor_tuned_RS)\n",
    "print(\"DNN tuned grid: %f\" % score_regressor_tuned_GS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same picture with the DNN Regressor. The tueing helps, but the results are still underwelming. Also the best DNN result is no match for the tuned SVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement  \n",
    "The count of rented bikes (cnt) is just the sum of the features casual and registered. Two seperate models are trained to predict these features. And add them up afterwards. This should improve the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns:\n",
      "Index([u'instant', u'dteday', u'season', u'yr', u'mnth', u'holiday',\n",
      "       u'weekday', u'workingday', u'weathersit', u'temp', u'atemp', u'hum',\n",
      "       u'windspeed'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column:\n",
      "casual\n",
      "\n",
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid=[{'kernel': ['linear', 'rbf'], 'C': [100, 1000, 10000]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)\n",
      "\n",
      "Best parameter from grid search: {'kernel': 'linear', 'C': 1000}\n"
     ]
    }
   ],
   "source": [
    "#SVR for casual with with GridSearch - for casual users\n",
    "\n",
    "# Extracting\n",
    "feature_cols_cas = bike_data.columns[:-3]  # all columns but last are features\n",
    "target_col_cas = bike_data.columns[-3]  # last column is the target\n",
    "print (\"Feature columns:\\n{}\\n\".format(feature_cols_cas))\n",
    "print (\"Target column:\\n{}\\n\".format(target_col_cas))\n",
    "\n",
    "# Pre-processing\n",
    "X_cas = bike_data[feature_cols_cas.drop(['dteday'],['instant'])]  # feature values \n",
    "y_cas = bike_data[target_col_cas]  # corresponding targets\n",
    "\n",
    "# Split Set\n",
    "X_train_cas, X_test_cas, y_train_cas, y_test_cas = train_test_split(X_cas, y_cas)# test size is set to 0.25\n",
    "\n",
    "# Tuning SVR\n",
    "param_grid = [\n",
    "             {'C': [100, 1000, 10000],\n",
    "              'kernel': ['linear', 'rbf']}\n",
    "             ]\n",
    "\n",
    "# MSR optimized\n",
    "#svr_tuned_cas = GridSearchCV(SVR (C=1), param_grid = param_grid, scoring = 'mean_squared_error')\n",
    "\n",
    "# R^2 optimized\n",
    "svr_tuned_cas = GridSearchCV(SVR (C=1), param_grid = param_grid, scoring = 'r2')\n",
    "\n",
    "# Fitting\n",
    "svr_tuned_cas.fit(X_train_cas, y_train_cas)\n",
    "\n",
    "print (svr_tuned_cas)\n",
    "print ('\\n' \"Best parameter from grid search: {}\".format(svr_tuned_cas.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):\n",
      "Index([u'instant', u'dteday', u'season', u'yr', u'mnth', u'holiday',\n",
      "       u'weekday', u'workingday', u'weathersit', u'temp', u'atemp', u'hum',\n",
      "       u'windspeed'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column:\n",
      "registered\n",
      "\n",
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid=[{'kernel': ['linear', 'rbf'], 'C': [100, 1000, 10000]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)\n",
      "\n",
      "Best parameter from grid search:{'kernel': 'linear', 'C': 1000}\n"
     ]
    }
   ],
   "source": [
    "#SVR for casual with with RandomizesSearch - for registered users\n",
    "\n",
    "# Extracting\n",
    "feature_cols_reg = bike_data.columns[:-3]  # all columns but last are features\n",
    "target_col_reg = bike_data.columns[-2]  # last column is the target\n",
    "print (\"Feature column(s):\\n{}\\n\".format(feature_cols_reg))\n",
    "print (\"Target column:\\n{}\\n\".format(target_col_reg))\n",
    "\n",
    "# Pre-processing\n",
    "X_reg = bike_data[feature_cols_reg.drop(['dteday'],['casual'])]  # feature values \n",
    "y_reg = bike_data[target_col_reg]  # corresponding targets\n",
    "\n",
    "# Split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg)# test size is set to 0.25\n",
    "\n",
    "# Tuning SVR\n",
    "param_grid = [\n",
    "             {'C': [100, 1000, 10000],\n",
    "              'kernel': ['linear', 'rbf']}\n",
    "             ]\n",
    "\n",
    "#svr_tuned_reg = GridSearchCV(SVR (C=1), param_grid = param_grid, scoring = 'mean_squared_error')\n",
    "svr_tuned_reg = GridSearchCV(SVR (C=1), param_grid = param_grid, scoring = 'r2')\n",
    "\n",
    "\n",
    "# Fitting \n",
    "svr_tuned_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "print (svr_tuned_reg)\n",
    "print ('\\n' \"Best parameter from grid search:{}\".format(svr_tuned_reg.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score cas: 0.665269\n",
      "Score reg: 0.823346\n",
      "Score sum: 0.793619\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "\n",
    "#print ('Score cas: {0:f}'.format(mean_squared_error(y_test_cas,svr_tuned_cas.predict(X_test_cas))))\n",
    "#print ('Score reg: {0:f}'.format(mean_squared_error(y_test_reg,svr_tuned_reg.predict(X_test_reg))))\n",
    "print ('Score cas: {0:f}'.format(r2_score(y_test_cas,svr_tuned_cas.predict(X_test_cas))))\n",
    "print ('Score reg: {0:f}'.format(r2_score(y_test_reg,svr_tuned_reg.predict(X_test_reg))))\n",
    "\n",
    "predict_sum = svr_tuned_cas.predict(X_test) + svr_tuned_reg.predict(X_test)\n",
    "\n",
    "#score = mean_squared_error(y_test, predict_sum)\n",
    "score = r2_score(y_test, predict_sum)\n",
    "\n",
    "print('Score sum: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free-Form Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
