{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capstone Project\n",
    "Machine Learning Engineer Nanodegree\n",
    "\n",
    "# Demand prediction for a bike sharing systems\n",
    "\n",
    "Mai 2016  \n",
    "Philipp Vogler  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview  \n",
    "\n",
    "- Looked for a problem in the area of transport and logistics that is solvable with machine learning\n",
    "- Utelizing machine learning to forecast the demand for the washington DC bike sharing system 'capital bike share'  \n",
    "- Using different types of regression to find an algorithem to predict the demand for bikes based on calenderic and weather information.  \n",
    "- Weather, calendaric and demand information is provided in a dataset by the University of Porto at UCI ML Repository.  \n",
    "- This project tries to create a forecasting function based on two years of historic data by utelizing the machine learning libraries scikit-learn and tensor-flow.  \n",
    "\n",
    "> http://www.capitalbikeshare.com   \n",
    "> http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset  \n",
    "> http://freemeteo.de/wetter/  \n",
    "> http://dchr.dc.gov/page/holiday-schedules  \n",
    "> http://scikit-learn.org/stable/  \n",
    "> https://www.tensorflow.org  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement  \n",
    "\n",
    "The goal is to forecast the demand of bikes in in dependency of weather conditions like outside temperature and calendaric informations e.g. holidays. These information and the demand structure is provided in a set with two years of dayly historic data.  \n",
    "The demand is given as the total dayly demand and as a split for registered users and casual users. To increase the quality of the prediction registered user demand and casual user demand will be predicted seperatly in step two.  \n",
    "To make predictions machnie learning is used to train regressors. Scikit-Learn recomands a support vector regressor (SVR) for this kind of problem and dataset. In addition a deep neuronal network (DNN) regressor is trained for comparison. To find the hyperparameters for these regressors grid search and ramdomized search are utelized. Due to the small dataset cross validation is applied.  \n",
    "\n",
    "> http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR  \n",
    "> https://github.com/tensorflow/skflow/blob/master/g3doc/api_docs/python/estimators.md  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "To mesure the performance of the regressions two standard regression metrics are used: Mean squared eror (MSE) and the coefficient of determination (R^2). Both metrics are calculated for both regressor types. For comparison and parameter tuneing only R^2 is used due to the better readability.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from tensorflow.contrib import skflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fetching Dataset\n",
    "\n",
    "bike_data = pd.read_csv(\"day.csv\")\n",
    "\n",
    "print \"Data read successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):\n",
      "Index([u'instant', u'dteday', u'season', u'yr', u'mnth', u'holiday',\n",
      "       u'weekday', u'workingday', u'weathersit', u'temp', u'atemp', u'hum',\n",
      "       u'windspeed'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column:\n",
      "cnt\n"
     ]
    }
   ],
   "source": [
    "# Extracting\n",
    "\n",
    "feature_cols = bike_data.columns[:-3]  # all columns but last are features\n",
    "target_col = bike_data.columns[-1]  # last column is the target\n",
    "\n",
    "print (\"Feature column(s):\\n{}\\n\".format(feature_cols))\n",
    "print (\"Target column:\\n{}\".format(target_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Data values:\n",
      "   instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01       1   0     1        0        6           0   \n",
      "1        2  2011-01-02       1   0     1        0        0           0   \n",
      "2        3  2011-01-03       1   0     1        0        1           1   \n",
      "3        4  2011-01-04       1   0     1        0        2           1   \n",
      "4        5  2011-01-05       1   0     1        0        3           1   \n",
      "\n",
      "   weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
      "0           2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
      "1           2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
      "2           1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
      "3           1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
      "4           1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
      "\n",
      "    cnt  \n",
      "0   985  \n",
      "1   801  \n",
      "2  1349  \n",
      "3  1562  \n",
      "4  1600  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>366.000000</td>\n",
       "      <td>2.496580</td>\n",
       "      <td>0.500684</td>\n",
       "      <td>6.519836</td>\n",
       "      <td>0.028728</td>\n",
       "      <td>2.997264</td>\n",
       "      <td>0.683995</td>\n",
       "      <td>1.395349</td>\n",
       "      <td>0.495385</td>\n",
       "      <td>0.474354</td>\n",
       "      <td>0.627894</td>\n",
       "      <td>0.190486</td>\n",
       "      <td>848.176471</td>\n",
       "      <td>3656.172367</td>\n",
       "      <td>4504.348837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>211.165812</td>\n",
       "      <td>1.110807</td>\n",
       "      <td>0.500342</td>\n",
       "      <td>3.451913</td>\n",
       "      <td>0.167155</td>\n",
       "      <td>2.004787</td>\n",
       "      <td>0.465233</td>\n",
       "      <td>0.544894</td>\n",
       "      <td>0.183051</td>\n",
       "      <td>0.162961</td>\n",
       "      <td>0.142429</td>\n",
       "      <td>0.077498</td>\n",
       "      <td>686.622488</td>\n",
       "      <td>1560.256377</td>\n",
       "      <td>1937.211452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059130</td>\n",
       "      <td>0.079070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022392</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>183.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337083</td>\n",
       "      <td>0.337842</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.134950</td>\n",
       "      <td>315.500000</td>\n",
       "      <td>2497.000000</td>\n",
       "      <td>3152.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>366.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498333</td>\n",
       "      <td>0.486733</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.180975</td>\n",
       "      <td>713.000000</td>\n",
       "      <td>3662.000000</td>\n",
       "      <td>4548.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>548.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.655417</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>0.730209</td>\n",
       "      <td>0.233214</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>4776.500000</td>\n",
       "      <td>5956.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.861667</td>\n",
       "      <td>0.840896</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>3410.000000</td>\n",
       "      <td>6946.000000</td>\n",
       "      <td>8714.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          instant      season          yr        mnth     holiday     weekday  \\\n",
       "count  731.000000  731.000000  731.000000  731.000000  731.000000  731.000000   \n",
       "mean   366.000000    2.496580    0.500684    6.519836    0.028728    2.997264   \n",
       "std    211.165812    1.110807    0.500342    3.451913    0.167155    2.004787   \n",
       "min      1.000000    1.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "25%    183.500000    2.000000    0.000000    4.000000    0.000000    1.000000   \n",
       "50%    366.000000    3.000000    1.000000    7.000000    0.000000    3.000000   \n",
       "75%    548.500000    3.000000    1.000000   10.000000    0.000000    5.000000   \n",
       "max    731.000000    4.000000    1.000000   12.000000    1.000000    6.000000   \n",
       "\n",
       "       workingday  weathersit        temp       atemp         hum   windspeed  \\\n",
       "count  731.000000  731.000000  731.000000  731.000000  731.000000  731.000000   \n",
       "mean     0.683995    1.395349    0.495385    0.474354    0.627894    0.190486   \n",
       "std      0.465233    0.544894    0.183051    0.162961    0.142429    0.077498   \n",
       "min      0.000000    1.000000    0.059130    0.079070    0.000000    0.022392   \n",
       "25%      0.000000    1.000000    0.337083    0.337842    0.520000    0.134950   \n",
       "50%      1.000000    1.000000    0.498333    0.486733    0.626667    0.180975   \n",
       "75%      1.000000    2.000000    0.655417    0.608602    0.730209    0.233214   \n",
       "max      1.000000    3.000000    0.861667    0.840896    0.972500    0.507463   \n",
       "\n",
       "            casual   registered          cnt  \n",
       "count   731.000000   731.000000   731.000000  \n",
       "mean    848.176471  3656.172367  4504.348837  \n",
       "std     686.622488  1560.256377  1937.211452  \n",
       "min       2.000000    20.000000    22.000000  \n",
       "25%     315.500000  2497.000000  3152.000000  \n",
       "50%     713.000000  3662.000000  4548.000000  \n",
       "75%    1096.000000  4776.500000  5956.000000  \n",
       "max    3410.000000  6946.000000  8714.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploration\n",
    "\n",
    "print \"\\n Data values:\"\n",
    "print bike_data.head()  # print the first 5 rows\n",
    "\n",
    "print \"\\n Data stats:\"\n",
    "bike_data.describe() # shows stats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics\n",
    "\n",
    "Most of the data is already normalized or binary.  \n",
    "The dataset is very concise and missing values are not a problem. Categorical data like weekday or workingday are already processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing (Methodology)\n",
    "\n",
    "Dates get droped because the regressor can not read this datatype and the order information is already stored in the index. The instant variable replicates this information also. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "\n",
    "X = bike_data[feature_cols.drop(['dteday'],['instant'])] # feature values \n",
    "y = bike_data[target_col]  # corresponding targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "\n",
    "The visualiation shows a classic seasonal pattern with a up trend year over year. There are so outliers. These are left in the dataset because they are not due to mesurement errors, but to extrem weather conditions. Extrem weather conditions are part of the problem so the data is not excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visulazation\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(1)\n",
    "      \n",
    "plt.plot(bike_data.cnt,'go')\n",
    "#plt.plot(bike_data.casual,'yx')\n",
    "#plt.plot(bike_data.registered,'bx')\n",
    "\n",
    "plt.title('Number of bikes rented per day')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number of bikes')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# source: http://matplotlib.org/examples/showcase/bachelors_degrees_by_gender.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)# test size is set to 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of regressors are trained. A SVR and a DNN-Regressor. Both are first used \"of the shelf\" with default parameters to create a benchmark.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR  \n",
    "> https://github.com/tensorflow/skflow/blob/master/g3doc/api_docs/python/estimators.md  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both \"benchmarks\" for the coefficient of determination are very low. Parameter tuneing is mandatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training SVR\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score SVR: 0.000482\n"
     ]
    }
   ],
   "source": [
    "# Validation SVR\n",
    "\n",
    "svr_pred = svr.predict(X_test)\n",
    "\n",
    "# score_svr = mean_squared_error(y_test, svr_pred)\n",
    "score_svr = r2_score(y_test, svr_pred)\n",
    "\n",
    "print(\"Score SVR: %f\" % score_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #99, avg. train loss: 3475705.00000\n",
      "Step #199, avg. train loss: 2000593.87500\n",
      "Step #299, avg. train loss: 2129308.50000\n",
      "Step #399, avg. train loss: 2617349.50000\n",
      "Step #499, avg. train loss: 2149781.00000\n",
      "Step #600, epoch #1, avg. train loss: 2001944.00000\n",
      "Step #700, epoch #1, avg. train loss: 2128975.75000\n",
      "Step #800, epoch #1, avg. train loss: 2106880.25000\n",
      "Step #900, epoch #1, avg. train loss: 2099041.25000\n",
      "Step #1000, epoch #1, avg. train loss: 1477907.62500\n",
      "Step #1100, epoch #2, avg. train loss: 2811893.00000\n",
      "Step #1200, epoch #2, avg. train loss: 2109668.50000\n",
      "Step #1300, epoch #2, avg. train loss: 1854669.00000\n",
      "Step #1400, epoch #2, avg. train loss: 2182509.00000\n",
      "Step #1500, epoch #2, avg. train loss: 1442949.87500\n",
      "Step #1600, epoch #2, avg. train loss: 2523864.75000\n",
      "Step #1700, epoch #3, avg. train loss: 1882376.00000\n",
      "Step #1800, epoch #3, avg. train loss: 1453228.00000\n",
      "Step #1900, epoch #3, avg. train loss: 2197455.50000\n",
      "Step #2000, epoch #3, avg. train loss: 2259222.50000\n",
      "Step #2100, epoch #3, avg. train loss: 2239009.50000\n",
      "Step #2200, epoch #4, avg. train loss: 2009621.75000\n",
      "Step #2300, epoch #4, avg. train loss: 1991322.25000\n",
      "Step #2400, epoch #4, avg. train loss: 1870255.37500\n",
      "Step #2500, epoch #4, avg. train loss: 1936191.62500\n",
      "Step #2600, epoch #4, avg. train loss: 1597665.25000\n",
      "Step #2700, epoch #4, avg. train loss: 2169564.75000\n",
      "Step #2800, epoch #5, avg. train loss: 1774549.75000\n",
      "Step #2900, epoch #5, avg. train loss: 1844618.50000\n",
      "Step #3000, epoch #5, avg. train loss: 1757088.37500\n",
      "Step #3100, epoch #5, avg. train loss: 2051161.50000\n",
      "Step #3200, epoch #5, avg. train loss: 2109528.75000\n",
      "Step #3300, epoch #6, avg. train loss: 2193321.00000\n",
      "Step #3400, epoch #6, avg. train loss: 1773133.25000\n",
      "Step #3500, epoch #6, avg. train loss: 2003590.37500\n",
      "Step #3600, epoch #6, avg. train loss: 1735131.37500\n",
      "Step #3700, epoch #6, avg. train loss: 2099975.00000\n",
      "Step #3800, epoch #6, avg. train loss: 1504234.75000\n",
      "Step #3900, epoch #7, avg. train loss: 2082260.00000\n",
      "Step #4000, epoch #7, avg. train loss: 1463665.62500\n",
      "Step #4100, epoch #7, avg. train loss: 2035207.37500\n",
      "Step #4200, epoch #7, avg. train loss: 1777865.87500\n",
      "Step #4300, epoch #7, avg. train loss: 1520972.50000\n",
      "Step #4400, epoch #8, avg. train loss: 2176726.50000\n",
      "Step #4500, epoch #8, avg. train loss: 1885659.87500\n",
      "Step #4600, epoch #8, avg. train loss: 1259911.00000\n",
      "Step #4700, epoch #8, avg. train loss: 2573793.50000\n",
      "Step #4800, epoch #8, avg. train loss: 1585925.25000\n",
      "Step #4900, epoch #8, avg. train loss: 1349332.62500\n",
      "Step #5000, epoch #9, avg. train loss: 1616818.50000\n",
      "Score: -0.236513\n"
     ]
    }
   ],
   "source": [
    "# DNN-Regressor\n",
    "\n",
    "# Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
    "regressor = skflow.TensorFlowDNNRegressor(hidden_units=[10,10], steps=5000, learning_rate=0.1, batch_size=1)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and validate\n",
    "#score_regressor = metrics.mean_squared_error( y_test, regressor.predict(X_test))\n",
    "score_regressor = r2_score(y_test, regressor.predict(X_test))\n",
    "\n",
    "print('\\n Score: {0:f}'.format(score_regressor))\n",
    "\n",
    "#  Copyright 2015-present The Scikit Flow Authors. All Rights Reserved.\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# source https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/boston.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The regressors are trained using randomised search and cross validation to identify the area of the best parameters. Than a grid search is used to tune parameter values of the regressor functions.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid=[{'kernel': ['linear', 'rbf'], 'C': [1000, 3000, 10000]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)\n",
      "\n",
      "Best parameter from grid search: {'kernel': 'linear', 'C': 3000}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tuning SVR with GridSearch\n",
    "\n",
    "tuned_parameters = [{'C': [1000, 3000, 10000], \n",
    "                     'kernel': ['linear', 'rbf']}\n",
    "                   ]\n",
    "\n",
    "#svr_tuned = GridSearchCV(SVR (C=1), param_grid = tuned_parameters, scoring = 'mean_squared_error') #default 3-fold cross-validation, score method of the estimator\n",
    "svr_tuned_GS = GridSearchCV(SVR (C=1), param_grid = tuned_parameters, scoring = 'r2', n_jobs=-1) #default 3-fold cross-validation, score method of the estimator\n",
    "\n",
    "svr_tuned_GS.fit(X_train, y_train)\n",
    "\n",
    "print (svr_tuned_GS)\n",
    "print ('\\n' \"Best parameter from grid search: \" + str(svr_tuned_GS.best_params_) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Results\n",
      "\n",
      "Score SVR: 0.000482\n",
      "Score SVR tuned GS: 0.757696\n"
     ]
    }
   ],
   "source": [
    "# Validation - SVR tuned \n",
    "\n",
    "svr_tuned_pred_GS = svr_tuned_GS.predict(X_test)\n",
    "\n",
    "#score_svr_tuned = mean_squared_error(y_test, svr_tuned_pred)\n",
    "score_svr_tuned_GS = r2_score(y_test, svr_tuned_pred_GS)\n",
    "\n",
    "print('SVR Results\\n')\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"Score SVR tuned GS: %f\" % score_svr_tuned_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best CV score from grid search: 0.783958\n",
      "corresponding parameters: {'kernel': 'linear', 'C': 3548.376157985727}\n"
     ]
    }
   ],
   "source": [
    "# SVR tuned with RandomizesSearch\n",
    "# may take a while!\n",
    "\n",
    "# Parameters\n",
    "param_dist = {  'C': sp_uniform (1000, 10000), \n",
    "                'kernel': ['linear']\n",
    "             }\n",
    "\n",
    "n_iter_search = 1\n",
    "\n",
    "# MSE optimized\n",
    "#SVR_tuned_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'mean_squared_error', n_iter=n_iter_search)\n",
    "\n",
    "# R^2 optimized\n",
    "SVR_tuned_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'r2', n_iter=n_iter_search)\n",
    "\n",
    "# Fit\n",
    "SVR_tuned_RS.fit(X_train, y_train)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(SVR_tuned_RS.best_score_))\n",
    "print('corresponding parameters: {}'.format(SVR_tuned_RS.best_params_))\n",
    "\n",
    "# Predict and score\n",
    "predict = SVR_tuned_RS.predict(X_test)\n",
    "\n",
    "#score_regressor_tuned_RS = mean_squared_error(y_test, predict)\n",
    "score_svr_tuned_RS = r2_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Results\n",
      "\n",
      "Score SVR: 0.000482\n",
      "Score SVR tuned GS: 0.757696\n",
      "Score SVR tuned RS: 0.756611\n"
     ]
    }
   ],
   "source": [
    "print('SVR Results\\n')\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"Score SVR tuned GS: %f\" % score_svr_tuned_GS)\n",
    "print(\"Score SVR tuned RS: %f\" % score_svr_tuned_RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuneing works for the SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100, epoch #50, avg. train loss: 2010684.00000\n",
      "Step #100, epoch #50, avg. train loss: 1978598.50000\n",
      "Step #100, epoch #50, avg. train loss: 2005605.12500\n",
      "Step #100, epoch #50, avg. train loss: 2649574.50000\n",
      "Step #100, epoch #50, avg. train loss: 2565630.75000\n",
      "Step #100, epoch #50, avg. train loss: 2564813.50000\n",
      "Step #100, epoch #50, avg. train loss: 2663323.75000\n",
      "Step #100, epoch #50, avg. train loss: 2601203.00000\n",
      "Step #100, epoch #50, avg. train loss: 2707351.75000\n",
      "Step #100, epoch #50, avg. train loss: 1990417.25000\n",
      "Step #100, epoch #50, avg. train loss: 1953482.25000\n",
      "Step #100, epoch #50, avg. train loss: 2029625.12500\n",
      "Step #100, epoch #50, avg. train loss: 2498554.25000\n",
      "Step #100, epoch #50, avg. train loss: 2420813.00000\n",
      "Step #100, epoch #50, avg. train loss: 2468398.00000\n",
      "Step #100, epoch #50, avg. train loss: 2943557.50000\n",
      "Step #100, epoch #50, avg. train loss: 2864365.50000\n",
      "Step #100, epoch #50, avg. train loss: 2871355.25000\n",
      "Step #100, epoch #50, avg. train loss: 2053932.37500\n",
      "Step #100, epoch #50, avg. train loss: 2008846.12500\n",
      "Step #100, epoch #50, avg. train loss: 2067015.87500\n",
      "Step #100, epoch #50, avg. train loss: 2201853.00000\n",
      "Step #100, epoch #50, avg. train loss: 2135659.25000\n",
      "Step #100, epoch #50, avg. train loss: 2197872.25000\n",
      "Step #100, epoch #50, avg. train loss: 3953336.75000\n",
      "Step #100, epoch #50, avg. train loss: 3773842.25000\n",
      "Step #100, epoch #50, avg. train loss: 3766207.75000\n",
      "Step #100, epoch #50, avg. train loss: 2017787.25000\n",
      "Step #100, epoch #50, avg. train loss: 1949963.00000\n",
      "Step #100, epoch #50, avg. train loss: 2056333.87500\n",
      "Step #100, epoch #50, avg. train loss: 2891934.75000\n",
      "Step #100, epoch #50, avg. train loss: 2633473.50000\n",
      "Step #100, epoch #50, avg. train loss: 2852204.25000\n",
      "Step #100, epoch #50, avg. train loss: 3014191.75000\n",
      "Step #100, epoch #50, avg. train loss: 2764086.00000\n",
      "Step #100, epoch #50, avg. train loss: 3022397.50000\n",
      "Step #100, epoch #50, avg. train loss: 1997088.75000\n",
      "Step #100, epoch #50, avg. train loss: 1961902.12500\n",
      "Step #100, epoch #50, avg. train loss: 2033617.12500\n",
      "Step #100, epoch #50, avg. train loss: 3057426.50000\n",
      "Step #100, epoch #50, avg. train loss: 2897505.00000\n",
      "Step #100, epoch #50, avg. train loss: 2953542.75000\n",
      "Step #100, epoch #50, avg. train loss: 3466642.25000\n",
      "Step #100, epoch #50, avg. train loss: 3104630.50000\n",
      "Step #100, epoch #50, avg. train loss: 2854046.50000\n",
      "Step #100, epoch #50, avg. train loss: 2001029.87500\n",
      "Step #100, epoch #50, avg. train loss: 2001893.75000\n",
      "Step #100, epoch #50, avg. train loss: 2040284.75000\n",
      "Step #100, epoch #50, avg. train loss: 2603919.00000\n",
      "Step #100, epoch #50, avg. train loss: 2573348.50000\n",
      "Step #100, epoch #50, avg. train loss: 2660678.50000\n",
      "Step #100, epoch #50, avg. train loss: 2848485.50000\n",
      "Step #100, epoch #50, avg. train loss: 2772332.50000\n",
      "Step #100, epoch #50, avg. train loss: 2659195.25000\n",
      "Step #100, epoch #50, avg. train loss: 2007917.75000\n",
      "Step #100, epoch #50, avg. train loss: 1986563.37500\n",
      "Step #100, epoch #50, avg. train loss: 2031353.87500\n",
      "Step #100, epoch #50, avg. train loss: 2641561.00000\n",
      "Step #100, epoch #50, avg. train loss: 2509031.75000\n",
      "Step #100, epoch #50, avg. train loss: 2535963.75000\n",
      "Step #100, epoch #50, avg. train loss: 2936344.25000\n",
      "Step #100, epoch #50, avg. train loss: 2824691.00000\n",
      "Step #100, epoch #50, avg. train loss: 2872567.25000\n",
      "Step #100, epoch #50, avg. train loss: 2056031.00000\n",
      "Step #100, epoch #50, avg. train loss: 2018318.37500\n",
      "Step #100, epoch #50, avg. train loss: 2072798.50000\n",
      "Step #100, epoch #50, avg. train loss: 2200594.50000\n",
      "Step #100, epoch #50, avg. train loss: 2127055.25000\n",
      "Step #100, epoch #50, avg. train loss: 2178079.00000\n",
      "Step #100, epoch #50, avg. train loss: 4001354.00000\n",
      "Step #100, epoch #50, avg. train loss: 3917934.75000\n",
      "Step #100, epoch #50, avg. train loss: 3849965.75000\n",
      "Step #100, epoch #50, avg. train loss: 2022205.25000\n",
      "Step #100, epoch #50, avg. train loss: 1989608.37500\n",
      "Step #100, epoch #50, avg. train loss: 2048714.50000\n",
      "Step #100, epoch #50, avg. train loss: 2944263.00000\n",
      "Step #100, epoch #50, avg. train loss: 2793503.25000\n",
      "Step #100, epoch #50, avg. train loss: 2928051.50000\n",
      "Step #100, epoch #50, avg. train loss: 3004268.50000\n",
      "Step #100, epoch #50, avg. train loss: 2856915.25000\n",
      "Step #100, epoch #50, avg. train loss: 2928235.00000\n",
      "Step #100, epoch #50, avg. train loss: 2048840.12500\n",
      "Step #100, epoch #50, avg. train loss: 2013133.12500\n",
      "Step #100, epoch #50, avg. train loss: 2043750.50000\n",
      "Step #100, epoch #50, avg. train loss: 3098048.75000\n",
      "Step #100, epoch #50, avg. train loss: 2947468.50000\n",
      "Step #100, epoch #50, avg. train loss: 3033980.50000\n",
      "Step #100, epoch #50, avg. train loss: 3312476.25000\n",
      "Step #100, epoch #50, avg. train loss: 2492791.50000\n",
      "Step #100, epoch #50, avg. train loss: 3158605.75000\n",
      "Step #100, epoch #50, avg. train loss: 1952514.50000\n",
      "Step #100, epoch #50, avg. train loss: 1975223.87500\n",
      "Step #100, epoch #50, avg. train loss: 2043557.00000\n",
      "Step #100, epoch #50, avg. train loss: 2662970.75000\n",
      "Step #100, epoch #50, avg. train loss: 2603226.50000\n",
      "Step #100, epoch #50, avg. train loss: 2716028.25000\n",
      "Step #100, epoch #50, avg. train loss: 2707805.00000\n",
      "Step #100, epoch #50, avg. train loss: 2625032.25000\n",
      "Step #100, epoch #50, avg. train loss: 2772958.50000\n",
      "Step #100, epoch #50, avg. train loss: 1939385.87500\n",
      "Step #100, epoch #50, avg. train loss: 1946524.00000\n",
      "Step #100, epoch #50, avg. train loss: 2009260.12500\n",
      "Step #100, epoch #50, avg. train loss: 2525769.75000\n",
      "Step #100, epoch #50, avg. train loss: 2371257.50000\n",
      "Step #100, epoch #50, avg. train loss: 2700919.00000\n",
      "Step #100, epoch #50, avg. train loss: 5008949.00000\n",
      "Step #100, epoch #50, avg. train loss: 4314656.50000\n",
      "Step #100, epoch #50, avg. train loss: 2824886.50000\n",
      "Step #100, epoch #50, avg. train loss: 1936432.50000\n",
      "Step #100, epoch #50, avg. train loss: 1967176.00000\n",
      "Step #100, epoch #50, avg. train loss: 2033084.62500\n",
      "Step #100, epoch #50, avg. train loss: 2115305.00000\n",
      "Step #100, epoch #50, avg. train loss: 2062965.50000\n",
      "Step #100, epoch #50, avg. train loss: 2156429.00000\n",
      "Step #100, epoch #50, avg. train loss: 3827455.00000\n",
      "Step #100, epoch #50, avg. train loss: 3707220.25000\n",
      "Step #100, epoch #50, avg. train loss: 3990617.00000\n",
      "Step #100, epoch #50, avg. train loss: 1998086.50000\n",
      "Step #100, epoch #50, avg. train loss: 1953285.75000\n",
      "Step #100, epoch #50, avg. train loss: 1993171.25000\n",
      "Step #100, epoch #50, avg. train loss: 3891680.75000\n",
      "Step #100, epoch #50, avg. train loss: 2443843.00000\n",
      "Step #100, epoch #50, avg. train loss: 2516040.25000\n",
      "Step #100, epoch #50, avg. train loss: 2955122.25000\n",
      "Step #100, epoch #50, avg. train loss: 3006589.00000\n",
      "Step #100, epoch #50, avg. train loss: 3137270.50000\n",
      "Step #100, epoch #50, avg. train loss: 1999185.00000\n",
      "Step #100, epoch #50, avg. train loss: 2004277.12500\n",
      "Step #100, epoch #50, avg. train loss: 2106749.75000\n",
      "Step #100, epoch #50, avg. train loss: 3047835.50000\n",
      "Step #100, epoch #50, avg. train loss: 3008434.25000\n",
      "Step #100, epoch #50, avg. train loss: 3122440.25000\n",
      "Step #100, epoch #50, avg. train loss: 3438408.75000\n",
      "Step #100, epoch #50, avg. train loss: 2570993.75000\n",
      "Step #100, epoch #50, avg. train loss: 3222865.25000\n",
      "Step #100, epoch #100, avg. train loss: 1963679.25000\n",
      "Step #100, epoch #100, avg. train loss: 1949979.50000\n",
      "Step #100, epoch #100, avg. train loss: 2016670.37500\n",
      "Step #100, epoch #100, avg. train loss: 2552798.00000\n",
      "Step #100, epoch #100, avg. train loss: 2444267.50000\n",
      "Step #100, epoch #100, avg. train loss: 2555183.25000\n",
      "Step #100, epoch #100, avg. train loss: 2976821.00000\n",
      "Step #100, epoch #100, avg. train loss: 2516151.00000\n",
      "Step #100, epoch #100, avg. train loss: 2634846.50000\n",
      "Step #100, epoch #100, avg. train loss: 1971387.62500\n",
      "Step #100, epoch #100, avg. train loss: 1926485.75000\n",
      "Step #100, epoch #100, avg. train loss: 2017303.50000\n",
      "Step #100, epoch #100, avg. train loss: 2474544.75000\n",
      "Step #100, epoch #100, avg. train loss: 2372572.75000\n",
      "Step #100, epoch #100, avg. train loss: 2502891.75000\n",
      "Step #100, epoch #100, avg. train loss: 2895748.50000\n",
      "Step #100, epoch #100, avg. train loss: 2783519.25000\n",
      "Step #100, epoch #100, avg. train loss: 2879946.25000\n",
      "Step #100, epoch #100, avg. train loss: 2013173.50000\n",
      "Step #100, epoch #100, avg. train loss: 1982376.00000\n",
      "Step #100, epoch #100, avg. train loss: 2050878.75000\n",
      "Step #100, epoch #100, avg. train loss: 2170021.00000\n",
      "Step #100, epoch #100, avg. train loss: 2100309.50000\n",
      "Step #100, epoch #100, avg. train loss: 2189509.00000\n",
      "Step #100, epoch #100, avg. train loss: 3929819.00000\n",
      "Step #100, epoch #100, avg. train loss: 3669391.00000\n",
      "Step #100, epoch #100, avg. train loss: 3856236.50000\n",
      "Step #100, epoch #100, avg. train loss: 1998759.50000\n",
      "Step #100, epoch #100, avg. train loss: 1962699.50000\n",
      "Step #100, epoch #100, avg. train loss: 2029288.00000\n",
      "Step #100, epoch #100, avg. train loss: 2795921.00000\n",
      "Step #100, epoch #100, avg. train loss: 2740332.25000\n",
      "Step #100, epoch #100, avg. train loss: 2822605.50000\n",
      "Step #100, epoch #100, avg. train loss: 2878310.00000\n",
      "Step #100, epoch #100, avg. train loss: 2857955.50000\n",
      "Step #100, epoch #100, avg. train loss: 2885811.25000\n",
      "Step #100, epoch #100, avg. train loss: 1969172.12500\n",
      "Step #100, epoch #100, avg. train loss: 1977528.12500\n",
      "Step #100, epoch #100, avg. train loss: 2023117.50000\n",
      "Step #100, epoch #100, avg. train loss: 2992805.50000\n",
      "Step #100, epoch #100, avg. train loss: 2831652.75000\n",
      "Step #100, epoch #100, avg. train loss: 2941194.00000\n",
      "Step #100, epoch #100, avg. train loss: 3727868.50000\n",
      "Step #100, epoch #100, avg. train loss: 2503952.75000\n",
      "Step #100, epoch #100, avg. train loss: 2797383.00000\n",
      "Step #100, epoch #100, avg. train loss: 1963679.25000\n",
      "Step #100, epoch #100, avg. train loss: 1949979.50000\n",
      "Step #100, epoch #100, avg. train loss: 2016670.37500\n",
      "Step #100, epoch #100, avg. train loss: 2552798.00000\n",
      "Step #100, epoch #100, avg. train loss: 2444267.50000\n",
      "Step #100, epoch #100, avg. train loss: 2555183.25000\n",
      "Step #100, epoch #100, avg. train loss: 2976821.00000\n",
      "Step #100, epoch #100, avg. train loss: 2516151.00000\n",
      "Step #100, epoch #100, avg. train loss: 2634846.50000\n",
      "Step #100, epoch #100, avg. train loss: 1971387.62500\n",
      "Step #100, epoch #100, avg. train loss: 1926485.75000\n",
      "Step #100, epoch #100, avg. train loss: 2017303.50000\n",
      "Step #100, epoch #100, avg. train loss: 2474544.75000\n",
      "Step #100, epoch #100, avg. train loss: 2372572.75000\n",
      "Step #100, epoch #100, avg. train loss: 2502891.75000\n",
      "Step #100, epoch #100, avg. train loss: 2895748.50000\n",
      "Step #100, epoch #100, avg. train loss: 2783519.25000\n",
      "Step #100, epoch #100, avg. train loss: 2879946.25000\n",
      "Step #100, epoch #100, avg. train loss: 2013173.50000\n",
      "Step #100, epoch #100, avg. train loss: 1982376.00000\n",
      "Step #100, epoch #100, avg. train loss: 2050878.75000\n",
      "Step #100, epoch #100, avg. train loss: 2170021.00000\n",
      "Step #100, epoch #100, avg. train loss: 2100309.50000\n",
      "Step #100, epoch #100, avg. train loss: 2189509.00000\n",
      "Step #100, epoch #100, avg. train loss: 3929819.00000\n",
      "Step #100, epoch #100, avg. train loss: 3669391.00000\n",
      "Step #100, epoch #100, avg. train loss: 3856236.50000\n",
      "Step #100, epoch #100, avg. train loss: 1998759.50000\n",
      "Step #100, epoch #100, avg. train loss: 1962699.50000\n",
      "Step #100, epoch #100, avg. train loss: 2029288.00000\n",
      "Step #100, epoch #100, avg. train loss: 2795921.00000\n",
      "Step #100, epoch #100, avg. train loss: 2740332.25000\n",
      "Step #100, epoch #100, avg. train loss: 2822605.50000\n",
      "Step #100, epoch #100, avg. train loss: 2878310.00000\n",
      "Step #100, epoch #100, avg. train loss: 2857955.50000\n",
      "Step #100, epoch #100, avg. train loss: 2885811.25000\n",
      "Step #100, epoch #100, avg. train loss: 1969172.12500\n",
      "Step #100, epoch #100, avg. train loss: 1977528.12500\n",
      "Step #100, epoch #100, avg. train loss: 2023117.50000\n",
      "Step #100, epoch #100, avg. train loss: 2992805.50000\n",
      "Step #100, epoch #100, avg. train loss: 2831652.75000\n",
      "Step #100, epoch #100, avg. train loss: 2941194.00000\n",
      "Step #100, epoch #100, avg. train loss: 3727868.50000\n",
      "Step #100, epoch #100, avg. train loss: 2503952.75000\n",
      "Step #100, epoch #100, avg. train loss: 2797383.00000\n",
      "Step #100, epoch #50, avg. train loss: 3780466.50000\n",
      "best CV score from grid search: 0.253638\n",
      "corresponding parameters: {'steps': 100, 'learning_rate': 1.0, 'hidden_units': [13, 13], 'batch_size': 300}\n",
      "Score: -0.021819\n"
     ]
    }
   ],
   "source": [
    "# DNN-Regressor tuned with GS\n",
    "\n",
    "# param_grid\n",
    "param_grid = {'hidden_units': [[11,11], [12,12], [13,13], [14,14], [15,15]], \n",
    "              'steps': [100],\n",
    "              'learning_rate': [0.3, 0.7, 1.0],\n",
    "              'batch_size': [250, 300, 350, 400, 450]\n",
    "             }\n",
    "\n",
    "# GS with MSE\n",
    "#regressor_tuned = GridSearchCV(skflow.TensorFlowDNNRegressor (hidden_units = [10, 10]), param_grid, scoring = 'mean_squared_error')\n",
    "\n",
    "# GS with R^2\n",
    "regressor_tuned_GS = GridSearchCV(skflow.TensorFlowDNNRegressor (hidden_units = [10, 10]), param_grid, scoring = 'r2', n_jobs=-1)\n",
    "\n",
    "# Fit\n",
    "regressor_tuned_GS.fit(X_train, y_train)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(regressor_tuned_GS.best_score_))\n",
    "print('corresponding parameters: {}'.format(regressor_tuned_GS.best_params_))\n",
    "\n",
    "# source: https://github.com/tensorflow/skflow/pull/126/files\n",
    "\n",
    "# Predict and score\n",
    "predict = regressor_tuned_GS.predict(X_test)\n",
    "\n",
    "#score_regressor_tuned_GS = mean_squared_error(y_test, predict)\n",
    "score_regressor_tuned_GS = r2_score(y_test, predict)\n",
    "\n",
    "print('Score: {0:f}'.format(score_regressor_tuned_GS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Regressor Results\n",
      "\n",
      "DNN: -0.236513\n",
      "DNN tuned grid: -0.021819\n"
     ]
    }
   ],
   "source": [
    "print('DNN Regressor Results\\n')\n",
    "print(\"DNN: %f\" % score_regressor)\n",
    "print(\"DNN tuned grid: %f\" % score_regressor_tuned_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100, epoch #50, avg. train loss: 2038259.25000\n",
      "Step #200, epoch #100, avg. train loss: 1549159.25000\n",
      "Step #100, epoch #50, avg. train loss: 2031939.50000\n",
      "Step #200, epoch #100, avg. train loss: 1647863.87500\n",
      "Step #100, epoch #50, avg. train loss: 2057597.25000\n",
      "Step #200, epoch #100, avg. train loss: 1664315.37500\n",
      "Step #100, epoch #50, avg. train loss: 2046084.12500\n",
      "Step #200, epoch #100, avg. train loss: 1598422.25000\n",
      "\n",
      " best CV score from grid search: 0.180538\n",
      "\n",
      " corresponding parameters: {'learning_rate': 0.1844694727284093, 'hidden_units': [11, 11], 'batch_size': 316}\n",
      "\n",
      " Score: -0.073531\n"
     ]
    }
   ],
   "source": [
    "# DNN-Regressor tuned with RandomizesSearch\n",
    "\n",
    "# Parameters\n",
    "param_dist = {  'hidden_units': [[11,11], [12,12], [13,13]], \n",
    "                'learning_rate': sp_uniform(0.0,1.0), \n",
    "                'batch_size': sp_randint(250, 350)\n",
    "             }\n",
    "\n",
    "n_iter_search = 1\n",
    "\n",
    "# MSE optimized\n",
    "#regressor_tuned_RS = RandomizedSearchCV(skflow.TensorFlowDNNRegressor (hidden_units = [10, 10]), param_distributions = param_dist, scoring = 'mean_squared_error', n_iter=n_iter_search)\n",
    "\n",
    "# R^2 optimized\n",
    "regressor_tuned_RS = RandomizedSearchCV(skflow.TensorFlowDNNRegressor (hidden_units = [10, 10]), param_distributions = param_dist, scoring = 'r2', n_iter=n_iter_search)\n",
    "\n",
    "# Fit\n",
    "regressor_tuned_RS.fit(X_train, y_train)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('\\n best CV score from grid search: {0:f}'.format(regressor_tuned_RS.best_score_))\n",
    "print('\\n corresponding parameters: {}'.format(regressor_tuned_RS.best_params_))\n",
    "\n",
    "# source: https://github.com/tensorflow/skflow/pull/126/files\n",
    "\n",
    "# Predict and score\n",
    "predict = regressor_tuned_RS.predict(X_test)\n",
    "\n",
    "#score_regressor_tuned_RS = mean_squared_error(y_test, predict)\n",
    "score_regressor_tuned_RS = r2_score(y_test, predict)\n",
    "\n",
    "print('\\n Score: {0:f}'.format(score_regressor_tuned_RS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same picture with the DNN Regressor. The tuning helps, but the results are still underwelming. Also the best DNN result is no match for the tuned SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "\n",
      "SVR: 0.000482\n",
      "SVR tuned grid: 0.757696\n",
      "SVR tuned random: 0.756611\n",
      "\n",
      "\n",
      "DNN: -0.236513\n",
      "DNN tuned grid: -0.021819\n",
      "DNN tuned random: -0.073531\n"
     ]
    }
   ],
   "source": [
    "print('Results\\n')\n",
    "\n",
    "print(\"SVR: %f\" % score_svr)\n",
    "print(\"SVR tuned grid: %f\" % score_svr_tuned_GS)\n",
    "print(\"SVR tuned random: %f\" % score_svr_tuned_RS)\n",
    "\n",
    "print('\\n')\n",
    "print(\"DNN: %f\" % score_regressor)\n",
    "print(\"DNN tuned grid: %f\" % score_regressor_tuned_GS)\n",
    "print(\"DNN tuned random: %f\" % score_regressor_tuned_RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR works better than the DNN Regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement  \n",
    "The count of rented bikes (cnt) is just the sum of the features casual and registered. Two seperate models are trained to predict these features. And add them up afterwards. This should improve the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns:\n",
      "Index([u'instant', u'dteday', u'season', u'yr', u'mnth', u'holiday',\n",
      "       u'weekday', u'workingday', u'weathersit', u'temp', u'atemp', u'hum',\n",
      "       u'windspeed'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column:\n",
      "casual\n",
      "\n",
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid=[{'kernel': ['linear', 'rbf'], 'C': [1, 3, 10, 30, 100, 300, 1000, 3000]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)\n",
      "\n",
      "Best parameter from grid search: {'kernel': 'linear', 'C': 300}\n"
     ]
    }
   ],
   "source": [
    "#SVR with GridSearch - for casual users\n",
    "\n",
    "# Extracting\n",
    "feature_cols_cas = bike_data.columns[:-3]  # all columns but last are features\n",
    "target_col_cas = bike_data.columns[-3]  # last column is the target\n",
    "print (\"Feature columns:\\n{}\\n\".format(feature_cols_cas))\n",
    "print (\"Target column:\\n{}\\n\".format(target_col_cas))\n",
    "\n",
    "# Pre-processing\n",
    "X_cas = bike_data[feature_cols_cas.drop(['dteday'],['instant'])]  # feature values \n",
    "y_cas = bike_data[target_col_cas]  # corresponding targets\n",
    "\n",
    "# Split Set\n",
    "X_train_cas, X_test_cas, y_train_cas, y_test_cas = train_test_split(X_cas, y_cas)# test size is set to 0.25\n",
    "\n",
    "# Tuning SVR\n",
    "param_grid = [\n",
    "             {'C': [1, 3, 10, 30, 100, 300, 1000, 3000],\n",
    "              'kernel': ['linear', 'rbf']}\n",
    "             ]\n",
    "\n",
    "# MSR optimized\n",
    "#svr_tuned_cas = GridSearchCV(SVR (C=1), param_grid = param_grid, scoring = 'mean_squared_error')\n",
    "\n",
    "# R^2 optimized\n",
    "svr_tuned_cas_GS = GridSearchCV(SVR (C=1), param_grid = param_grid, scoring = 'r2', n_jobs=-1)\n",
    "\n",
    "# Fitting\n",
    "svr_tuned_cas_GS.fit(X_train_cas, y_train_cas)\n",
    "\n",
    "print (svr_tuned_cas_GS)\n",
    "print ('\\n' \"Best parameter from grid search: {}\".format(svr_tuned_cas_GS.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best CV score from grid search: 0.645595\n",
      "corresponding parameters: {'kernel': 'linear', 'C': 1092.914166471615}\n"
     ]
    }
   ],
   "source": [
    "#SVR with RandomizesSearch - for casual users\n",
    "# may take a while!\n",
    "\n",
    "# Parameters\n",
    "param_dist = {  'C': sp_uniform (300, 3000), \n",
    "                'kernel': ['linear']\n",
    "             }\n",
    "\n",
    "n_iter_search = 1\n",
    "\n",
    "svr_tuned_cas_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'r2', n_iter=n_iter_search)\n",
    "\n",
    "# Fit\n",
    "svr_tuned_cas_RS.fit(X_train_cas, y_train_cas)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(svr_tuned_cas_RS.best_score_))\n",
    "print('corresponding parameters: {}'.format(svr_tuned_cas_RS.best_params_))\n",
    "\n",
    "# Predict and score\n",
    "predict = svr_tuned_cas_RS.predict(X_test)\n",
    "\n",
    "#score_regressor_tuned_RS = mean_squared_error(y_test, predict)\n",
    "score_SVR_tuned_RS = r2_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):\n",
      "Index([u'instant', u'dteday', u'season', u'yr', u'mnth', u'holiday',\n",
      "       u'weekday', u'workingday', u'weathersit', u'temp', u'atemp', u'hum',\n",
      "       u'windspeed'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column:\n",
      "registered\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVR for casual with with GridSearch - for registered users\n",
    "\n",
    "# Extracting\n",
    "feature_cols_reg = bike_data.columns[:-3]  # all columns but last are features\n",
    "target_col_reg = bike_data.columns[-2]  # last column is the target\n",
    "print (\"Feature column(s):\\n{}\\n\".format(feature_cols_reg))\n",
    "print (\"Target column:\\n{}\\n\".format(target_col_reg))\n",
    "\n",
    "# Pre-processing\n",
    "X_reg = bike_data[feature_cols_reg.drop(['dteday'],['casual'])]  # feature values \n",
    "y_reg = bike_data[target_col_reg]  # corresponding targets\n",
    "\n",
    "# Split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg)# test size is set to 0.25\n",
    "\n",
    "# Tuning SVR\n",
    "param_grid = [\n",
    "             {'C': [1000, 3000, 10000],\n",
    "              'kernel': ['linear', 'rbf']}\n",
    "             ]\n",
    "\n",
    "#svr_tuned_reg = GridSearchCV(SVR (C=1), param_grid = param_grid, scoring = 'mean_squared_error')\n",
    "svr_tuned_reg_GS = GridSearchCV(SVR (C=1), param_grid = param_grid, scoring = 'r2', n_jobs=-1)\n",
    "\n",
    "\n",
    "# Fitting \n",
    "svr_tuned_reg_GS.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "print (svr_tuned_reg_GS)\n",
    "print ('\\n' \"Best parameter from grid search:{}\".format(svr_tuned_reg_GS.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best CV score from grid search: 0.645595\n",
      "corresponding parameters: {'kernel': 'linear', 'C': 1092.914166471615}\n"
     ]
    }
   ],
   "source": [
    "#SVR with RandomizesSearch - for registered users\n",
    "# may take a while!\n",
    "\n",
    "# Parameters\n",
    "param_dist = {  'C': sp_uniform (1000, 3000), \n",
    "                'kernel': ['linear']\n",
    "             }\n",
    "\n",
    "n_iter_search = 1\n",
    "\n",
    "svr_tuned_reg_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'r2', n_iter=n_iter_search)\n",
    "\n",
    "# Fit\n",
    "svr_tuned_reg_RS.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(svr_tuned_cas_RS.best_score_))\n",
    "print('corresponding parameters: {}'.format(svr_tuned_cas_RS.best_params_))\n",
    "\n",
    "# Predict and score\n",
    "predict = svr_tuned_reg_RS.predict(X_test)\n",
    "\n",
    "#score_regressor_tuned_RS = mean_squared_error(y_test, predict)\n",
    "score_SVR_tuned_reg_RS = r2_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score cas: 0.640793\n",
      "Score reg: 0.754691\n",
      "Score sum: 0.776282\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "\n",
    "#print ('Score cas: {0:f}'.format(mean_squared_error(y_test_cas,svr_tuned_cas.predict(X_test_cas))))\n",
    "#print ('Score reg: {0:f}'.format(mean_squared_error(y_test_reg,svr_tuned_reg.predict(X_test_reg))))\n",
    "print ('Score cas: {0:f}'.format(r2_score(y_test_cas,svr_tuned_cas_RS.predict(X_test_cas))))\n",
    "print ('Score reg: {0:f}'.format(r2_score(y_test_reg,svr_tuned_reg_RS.predict(X_test_reg))))\n",
    "\n",
    "predict_sum_test = svr_tuned_cas_RS.predict(X_test) + svr_tuned_reg_RS.predict(X_test)\n",
    "\n",
    "#score = mean_squared_error(y_test, predict_sum)\n",
    "score = r2_score(y_test, predict_sum_test)\n",
    "\n",
    "print('Score sum: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR: 0.000482\n",
      "SVR tuned grid: 0.757696\n",
      "Score SVR tuned RS: 0.756611\n",
      "DNN: -0.236513\n",
      "DNN tuned grid: -0.021819\n",
      "DNN tuned random: -0.073531\n",
      "\n",
      "\n",
      "SVR sum: 0.776282\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(\"SVR: %f\" % score_svr)\n",
    "print(\"SVR tuned grid: %f\" % score_svr_tuned_GS)\n",
    "print(\"SVR tuned RS: %f\" % score_svr_tuned_RS)\n",
    "print('\\n')\n",
    "print(\"DNN: %f\" % score_regressor)\n",
    "print(\"DNN tuned grid: %f\" % score_regressor_tuned_GS)\n",
    "print(\"DNN tuned random: %f\" % score_regressor_tuned_RS)\n",
    "print('\\n')\n",
    "print('SVR sum: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The SVR beats the DNN Regressor by far.  \n",
    "- The seperat prediction of casual and registerd customers increases the R^2 sligtly.  \n",
    "- More than 80% determination is a deacent result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free-Form Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visulazation\n",
    "\n",
    "predict_sum_all = svr_tuned_cas_RS.predict(X) + svr_tuned_reg_RS.predict(X)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(1)\n",
    "      \n",
    "plt.plot(bike_data.cnt,'go', label='truth')\n",
    "plt.plot(predict_sum_all,'bx', label='prediction')\n",
    "\n",
    "plt.title('Number of bikes rented per day')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number of bikes')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# source: http://matplotlib.org/examples/showcase/bachelors_degrees_by_gender.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I had high hopes for the DNN Regressor. It was kind of disappointing that it does not even come close. Maybe my tuneing was not right or it neads more data or computaional power.\n",
    "- utelizing grid and randomize search in a way that makes sens was a little tricky. It makes more sens to start with a broad grid search and than use randomized search on the given intervall, instead of vis a versa. It is also coputational more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More than 80% determination is a deacent result.  \n",
    "- The could possibly be increased by increasing iterations in training and the number of folds in the cross validation, at the expense of computing time.\n",
    "- chain multiple estimators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
